{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install html2text"
      ],
      "metadata": {
        "id": "qbHrTarbBI7W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b0dc0d9-fb67-4e18-b0ac-c989968b353b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting html2text\n",
            "  Downloading html2text-2025.4.15-py3-none-any.whl.metadata (4.1 kB)\n",
            "Downloading html2text-2025.4.15-py3-none-any.whl (34 kB)\n",
            "Installing collected packages: html2text\n",
            "Successfully installed html2text-2025.4.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community langchain-google-genai chromadb google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ-wUC-Wpnji",
        "outputId": "c51ba50d-fca0-4524-8f53-66903bfb4e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.68)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.1)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
            "INFO: pip is looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-generativeai\n",
            "  Downloading google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
            "  Downloading google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.7.2-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Downloading google_generativeai-0.7.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: pip is still looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_generativeai-0.7.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.6.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading google_generativeai-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.4.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.4.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.3.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading google_generativeai-0.2.2-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.176.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.35.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.35.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.35.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.56b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.33.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.5)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.35.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.35.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=1a6d07dbf5f5eda7c1368bf981dad47a24e404b120a69b863bf6dc306f4b95ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, filetype, durationpy, uvloop, python-dotenv, pybase64, overrides, opentelemetry-proto, mypy-extensions, mmh3, marshmallow, humanfriendly, httpx-sse, httptools, bcrypt, backoff, watchfiles, typing-inspect, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, pydantic-settings, opentelemetry-semantic-conventions, onnxruntime, kubernetes, dataclasses-json, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb, langchain-google-genai, langchain-community\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.15 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.10 filetype-1.2.0 httptools-0.6.4 httpx-sse-0.4.1 humanfriendly-10.0 kubernetes-33.1.0 langchain-community-0.3.27 langchain-google-genai-2.0.10 marshmallow-3.26.1 mmh3-5.1.0 mypy-extensions-1.1.0 onnxruntime-1.22.1 opentelemetry-api-1.35.0 opentelemetry-exporter-otlp-proto-common-1.35.0 opentelemetry-exporter-otlp-proto-grpc-1.35.0 opentelemetry-proto-1.35.0 opentelemetry-sdk-1.35.0 opentelemetry-semantic-conventions-0.56b0 overrides-7.7.0 posthog-5.4.0 pybase64-1.4.1 pydantic-settings-2.10.1 pypika-0.48.9 python-dotenv-1.1.1 typing-inspect-0.9.0 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jazc9Ixa5yv6"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import html2text\n",
        "import re\n",
        "import time\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import json\n",
        "from typing import Dict, List, Tuple, Optional, Set, Any\n",
        "from collections import deque\n",
        "import os\n",
        "from langchain.text_splitter import MarkdownTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Overview**"
      ],
      "metadata": {
        "id": "TzJdAxrFaodx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are crawling the web and implementing RAG (Retrieval-Augmented Generation)  to provide helpful care tips and informational content to people living with Parkinson's, based on the self-reported severity of their symptoms."
      ],
      "metadata": {
        "id": "--UowMTUa5C9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Care-Tips Based on Symptom Severity**\n",
        "\n",
        "| Symptom | Type of care-tips based on rating | Rating | Care-tip example based on symptom and severity |\n",
        "|---------|-----------------------------------|--------|-----------------------------------------------|\n",
        "| Pain | Educational | 1-2 | I'm glad you've been able to manage your pain. Exercising regularly and making sure to get adequate nutrition can go a long way when regulating pain.<br><br>Meditating can be a good substitution if you are in too much pain to exercise. |\n",
        "| | Basic Care | 3 | Warm packs may help control your pain. However, avoid electric heating pads as they can cause burns with prolonged use.<br><br>If your pain is due to acute injury, consider using a cold pack instead to reduce pain and swelling. This should typically not be done for > 20 minutes. |\n",
        "| | Advanced Care | 4-5 | Consider using the journal as a 'pain log' to describe when, where and what kind of pain you are feeling as well as what has or hasn't helped relieve your pain. This can help you understand the causes of your pain.<br><br>Sharing this information with your health providers can help them classify and treat your pain accurately. |\n",
        "| | Escalation | 5 (User rates 4-5 > 4 times) | I recommend you speak to your doctor or your nurse about the pain you are experiencing.<br><br>They can help you find the underlying cause of your pain. |\n",
        "| Light-headedness | Educational | 1-2 | You've noticed some occasional light-headedness but it hasn't been too disruptive.<br><br>Let's keep an eye on it and track when it happens. |\n",
        "| | Basic Care | 3 | You've been feeling light-headed more often, especially when standing.<br><br>Let's monitor how often it happens and make a note to talk to your care team if it continues. |\n",
        "| | Advanced Care | 4-5 | You're starting to feel light-headed more regularly, and it may be getting in the way of your day.<br><br>Keep tracking when it happens and bring it up with your care team so they're aware. |\n",
        "| | Escalation | 5 (User rates 4-5 > 4 times) | Light-headedness has been coming up often and might be affecting you when moving around.<br><br>Let's continue tracking when it happens and make sure this gets shared with your care team. |\n",
        "| Unusual Sweating | Educational | 1-2 | Glad to hear that you've been able to manage your sweating problems.<br><br>Remember that sweating problems occur as a regulatory function of your autonomic nervous system. |\n",
        "| | Basic Care | 3 | Unusual sweating is a common symptom experienced by many people with PD. Try to identify any foods that can cause excessive sweating and use an antiperspirant to control sweating and odour.<br><br>You can consult this article for more information |\n",
        "| | Advanced Care | 4-5 | People with PD often experience discomfort and uneasiness due to unusual sweating, but there are ways to manage it. Avoid wearing tight-fitting clothing, especially those made from nylon or silk.<br><br>Consider buying breathable socks and armpit shields to absorb sweat and moisture. |\n",
        "| | Escalation | 5 (User rates 4-5 > 4 times) | It appears that you have been experiencing constant issues with excessive sweating. It may be the right time to contact your care team or consult our care finder to support your experience better.<br><br>In the meantime, try avoiding hot or humid environments, crowded rooms, and stressful situations. |\n",
        "| Skin Changes | Educational | 1-2 | You may have noticed occasional skin changes but it hasn't been a regular concern yet.<br><br>Let's keep track of this just in case. |\n",
        "| | Basic Care | 3 | Skin changes like increased oiliness or irritation are happening more often.<br><br>Let's continue monitoring and sharing them with your care team. |\n",
        "| | Advanced Care | 4-5 | You may be noticing changes in your skin which isn't unusual in Parkinson's.<br><br>Let's keep tracking each episode and share this with your care team. |\n",
        "| | Escalation | 5 (User rates 4-5 > 4 times) | You're consistently noticing skin changes, a normal symptom of Parkinson's.<br><br>Track each episode's timing and severity to share with your care team. |"
      ],
      "metadata": {
        "id": "6yMXSEqohC4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Grabbing Reliable Parkinson's Content From the Web**"
      ],
      "metadata": {
        "id": "Y7yDzRnFBYSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Organization Name                                               | Country/Region            | Description                                      | Successfully Crawled |\n",
        "|------------------------------------------------------------------|---------------------------|--------------------------------------------------|----------------------|\n",
        "| Parkinson’s Foundation                                           | USA (Global reach)        | [parkinson.org](https://www.parkinson.org/)     | ✅                   |\n",
        "| Michael J. Fox Foundation for Parkinson’s Research              | USA (Global reach)        | [michaeljfox.org](https://www.michaeljfox.org/) | ✅                   |\n",
        "| American Parkinson Disease Association (APDA)                   | USA                       | [apdaparkinson.org](https://www.apdaparkinson.org/) | ✅               |\n",
        "| Parkinson Canada                                                | Canada                    | [parkinson.ca](https://www.parkinson.ca/)       | ✅                   |\n",
        "| European Parkinson’s Disease Association (EPDA)                 | Europe (Pan-European)     | [parkinsonseurope.org](https://parkinsonseurope.org/) | ❌               |\n",
        "| Parkinson’s UK                                                  | United Kingdom            | [parkinsons.org.uk](https://www.parkinsons.org.uk/) | ✅               |\n",
        "| Davis Phinney Foundation                                        | USA                       | [davisphinneyfoundation.org](https://davisphinneyfoundation.org/) | ❌         |\n",
        "| PMD Alliance                                                    | USA                       | [pmdalliance.org](https://www.pmdalliance.org/) | ✅                   |\n",
        "| ParkinsonNet                                                    | Netherlands               | [parkinsonnet.com](https://www.parkinsonnet.com/) | ✅               |\n"
      ],
      "metadata": {
        "id": "tPDgukjNXTB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unable to crawl ❌*Davis Phinney Foundation* & ❌*European Parkinson's Disease Association* due to HTTP 403 Forbidden errors. These websites are most likely blocking the scraper**"
      ],
      "metadata": {
        "id": "uSk4iq2OVfqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grab all main pages and secondary pages with a limit of 50 pages and 3 levels deep, only crawls pages within the same domain + URLs for videos and podcast ressources"
      ],
      "metadata": {
        "id": "-ptVs_PYE_wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Google Colab Secrets\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(\"✅ API key loaded from Colab secrets\")\n",
        "except Exception as e:\n",
        "    print(\"❌ Could not load API key from secrets. Please set up your API key.\")\n",
        "    print(\"Go to the left sidebar 🔑 Secrets tab and add 'GOOGLE_API_KEY' as a secret\")"
      ],
      "metadata": {
        "id": "wQRvofwiYkkU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43a54ae5-afc9-4177-e2a1-b7d8500c8aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ API key loaded from Colab secrets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Google Colab Secrets\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(\"✅ API key loaded from Colab secrets\")\n",
        "except Exception as e:\n",
        "    print(\"❌ Could not load API key from secrets. Please set up your API key.\")\n",
        "    print(\"Go to the left sidebar 🔑 Secrets tab and add 'GOOGLE_API_KEY' as a secret\")\n",
        "\n",
        "# =============================================================================\n",
        "# 1. TEXT PREPROCESSING AND CLEANING\n",
        "# =============================================================================\n",
        "\n",
        "def merge_hyphenated_words(text):\n",
        "    \"\"\"Merge words that are split by hyphens across lines.\"\"\"\n",
        "    return re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", text)\n",
        "\n",
        "def fix_newlines(text):\n",
        "    \"\"\"Replace single newlines with spaces, keep double newlines as paragraph breaks.\"\"\"\n",
        "    return re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", text)\n",
        "\n",
        "def remove_multiple_newlines(text):\n",
        "    \"\"\"Replace multiple consecutive newlines with single newlines.\"\"\"\n",
        "    return re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "\n",
        "def clean_markdown_artifacts(text):\n",
        "    \"\"\"Clean up markdown artifacts that might not be useful for RAG.\"\"\"\n",
        "    # Remove excessive markdown links that are just URLs\n",
        "    text = re.sub(r'\\[([^\\]]*)\\]\\([^)]*\\)', r'\\1', text)\n",
        "    # Clean up excessive asterisks and underscores\n",
        "    text = re.sub(r'\\*{3,}', '***', text)\n",
        "    text = re.sub(r'_{3,}', '___', text)\n",
        "    # Remove excessive whitespace\n",
        "    text = re.sub(r' {3,}', ' ', text)\n",
        "    return text\n",
        "\n",
        "def remove_navigation_elements(text):\n",
        "    \"\"\"Remove common navigation and footer elements.\"\"\"\n",
        "    # Remove common navigation phrases\n",
        "    nav_patterns = [\n",
        "        r'Skip to main content',\n",
        "        r'Skip to navigation',\n",
        "        r'Back to top',\n",
        "        r'Contact Us',\n",
        "        r'Privacy Policy',\n",
        "        r'Terms of Service',\n",
        "        r'Copyright ©.*',\n",
        "        r'All rights reserved.*'\n",
        "    ]\n",
        "    for pattern in nav_patterns:\n",
        "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Comprehensive text cleaning pipeline for Parkinson's content.\n",
        "\n",
        "    Args:\n",
        "        text (str): Raw text to be cleaned\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned text\n",
        "    \"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    cleaning_functions = [\n",
        "        merge_hyphenated_words,\n",
        "        fix_newlines,\n",
        "        remove_multiple_newlines,\n",
        "        clean_markdown_artifacts,\n",
        "        remove_navigation_elements\n",
        "    ]\n",
        "\n",
        "    for cleaning_function in cleaning_functions:\n",
        "        text = cleaning_function(text)\n",
        "\n",
        "    # Final cleanup\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# =============================================================================\n",
        "# 2. DOCUMENT CHUNKING AND PROCESSING\n",
        "# =============================================================================\n",
        "\n",
        "def text_to_docs(text: str, metadata: Dict[str, Any]) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Convert text to Document chunks with metadata.\n",
        "\n",
        "    Args:\n",
        "        text (str): Cleaned text content\n",
        "        metadata (dict): Metadata for the document\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: List of document chunks\n",
        "    \"\"\"\n",
        "    if not text or len(text.strip()) < 50:  # Skip very short content\n",
        "        return []\n",
        "\n",
        "    doc_chunks = []\n",
        "    # Use larger chunks for better context in RAG\n",
        "    text_splitter = MarkdownTextSplitter(\n",
        "        chunk_size=1500,  # Slightly smaller for better retrieval\n",
        "        chunk_overlap=200  # More overlap for better context preservation\n",
        "    )\n",
        "\n",
        "    chunks = text_splitter.split_text(text)\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        if len(chunk.strip()) < 100:  # Skip very small chunks\n",
        "            continue\n",
        "\n",
        "        # Enhanced metadata for better retrieval\n",
        "        chunk_metadata = metadata.copy()\n",
        "        chunk_metadata.update({\n",
        "            'chunk_id': i,\n",
        "            'chunk_length': len(chunk),\n",
        "            'total_chunks': len(chunks)\n",
        "        })\n",
        "\n",
        "        doc = Document(page_content=chunk, metadata=chunk_metadata)\n",
        "        doc_chunks.append(doc)\n",
        "\n",
        "    return doc_chunks\n",
        "\n",
        "def get_doc_chunks(text: str, metadata: Dict[str, Any]) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Process text and metadata to generate document chunks.\n",
        "\n",
        "    Args:\n",
        "        text (str): Raw text content\n",
        "        metadata (dict): Associated metadata\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: List of processed document chunks\n",
        "    \"\"\"\n",
        "    cleaned_text = clean_text(text)\n",
        "    if not cleaned_text:\n",
        "        return []\n",
        "\n",
        "    doc_chunks = text_to_docs(cleaned_text, metadata)\n",
        "    return doc_chunks\n",
        "\n",
        "# =============================================================================\n",
        "# 3. CHROMADB INITIALIZATION AND CONNECTION - MODIFIED FOR PERSISTENCE\n",
        "# =============================================================================\n",
        "\n",
        "def get_persistent_directory():\n",
        "    \"\"\"Get or create persistent directory for ChromaDB data.\"\"\"\n",
        "    # Create a persistent directory in your local Downloads folder\n",
        "    persist_dir = \"/content/drive/MyDrive/ChromaDB_Parkinson_Data\"\n",
        "\n",
        "    # For local development, use a local directory\n",
        "    if not os.path.exists(\"/content/drive\"):\n",
        "        persist_dir = os.path.expanduser(\"~/Downloads/ChromaDB_Parkinson_Data\")\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(persist_dir, exist_ok=True)\n",
        "    return persist_dir\n",
        "\n",
        "def show_persistence_notification(persist_dir, collection_name, total_docs):\n",
        "    \"\"\"Show a notification about data persistence.\"\"\"\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    notification = f\"\"\"\n",
        "    <div style=\"background-color: #d4edda; border: 1px solid #c3e6cb; color: #155724;\n",
        "                padding: 15px; border-radius: 5px; margin: 10px 0; font-family: Arial;\">\n",
        "        <h3 style=\"margin-top: 0;\">💾 Data Successfully Persisted!</h3>\n",
        "        <p><strong>📍 Location:</strong> {persist_dir}</p>\n",
        "        <p><strong>📚 Collection:</strong> {collection_name}</p>\n",
        "        <p><strong>📊 Documents:</strong> {total_docs} chunks stored</p>\n",
        "        <p><strong>🔄 Next Steps:</strong> Your data is now saved locally and ready for Google Drive upload!</p>\n",
        "        <hr style=\"border: 1px solid #c3e6cb;\">\n",
        "        <p style=\"margin-bottom: 0; font-size: 0.9em;\">\n",
        "            <strong>💡 Tip:</strong> You can now upload the entire folder to Google Drive for backup and sharing.\n",
        "        </p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(notification))\n",
        "    print(f\"🎉 SUCCESS: Data persisted to {persist_dir}\")\n",
        "\n",
        "def get_chroma_client(collection_name: str = \"parkinsons_knowledge_base\"):\n",
        "    \"\"\"\n",
        "    Initialize and return ChromaDB client with Google Embeddings and LOCAL PERSISTENCE.\n",
        "\n",
        "    Args:\n",
        "        collection_name (str): Name of the ChromaDB collection\n",
        "\n",
        "    Returns:\n",
        "        Chroma: Initialized ChromaDB vector store with persistence\n",
        "    \"\"\"\n",
        "    embedding_function = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "    # Get persistent directory\n",
        "    persist_dir = get_persistent_directory()\n",
        "\n",
        "    print(f\"📁 Persistent data ready at: {persist_dir}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 13. EXAMPLE USAGE AND TESTING - UPDATED FOR PERSISTENCE\n",
        "# =============================================================================\n",
        "\n",
        "def run_complete_pipeline():\n",
        "    \"\"\"\n",
        "    Run the complete pipeline to create and test the knowledge base with PERSISTENCE\n",
        "    \"\"\"\n",
        "    print(\"🚀 STARTING COMPLETE PARKINSON'S RAG PIPELINE WITH PERSISTENCE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Step 1: Create complete knowledge base with persistence\n",
        "    print(\"\\n📚 STEP 1: Creating complete knowledge base with LOCAL PERSISTENCE...\")\n",
        "    process_complete_knowledge_base()\n",
        "\n",
        "    # Step 2: Test the knowledge base\n",
        "    print(\"\\n🧪 STEP 2: Testing persistent knowledge base...\")\n",
        "    try:\n",
        "        # Test the complete knowledge base\n",
        "        inspector = inspect_chromadb(\"parkinsons_complete_kb\")\n",
        "\n",
        "        # Search for text content\n",
        "        print(\"\\n📄 Text Content Search:\")\n",
        "        text_results = inspector.search_documents(\"Parkinson's symptoms treatment\", k=3)\n",
        "        for i, result in enumerate(text_results[:3], 1):\n",
        "            if \"error\" not in result:\n",
        "                print(f\"   {i}. {result['organization']} - {result.get('content_type', 'text')}\")\n",
        "                print(f\"      {result['content_preview'][:100]}...\")\n",
        "\n",
        "        # Search for media content\n",
        "        print(\"\\n🎬 Media Content Search:\")\n",
        "        media_results = search_media_content(\"exercise therapy\", media_type=\"all\", k=3)\n",
        "        for i, result in enumerate(media_results[:3], 1):\n",
        "            print(f\"   {i}. [{result['type'].upper()}] {result['title']}\")\n",
        "            print(f\"      Organization: {result['organization']}\")\n",
        "            print(f\"      URL: {result['media_url']}\")\n",
        "\n",
        "        # Export sample for inspection\n",
        "        print(\"\\n💾 Exporting sample data to persistent directory...\")\n",
        "        inspector.export_sample_data(\"complete_knowledge_base_sample.json\", sample_size=100)\n",
        "\n",
        "        # Show Google Drive upload instructions\n",
        "        print(\"\\n📤 Preparing for Google Drive upload...\")\n",
        "        prepare_for_google_drive_upload()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Testing failed: {str(e)}\")\n",
        "\n",
        "    print(\"\\n🎉 Pipeline complete! Your persistent knowledge base is ready for use and Google Drive upload.\")\n",
        "\n",
        "# =============================================================================\n",
        "# 14. MOUNT GOOGLE DRIVE HELPER (OPTIONAL)\n",
        "# =============================================================================\n",
        "\n",
        "def mount_google_drive_and_setup():\n",
        "    \"\"\"\n",
        "    Mount Google Drive and set up persistent directory there (for Google Colab)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "        # Update the persistent directory function to use Google Drive\n",
        "        global get_persistent_directory\n",
        "        def get_persistent_directory():\n",
        "            persist_dir = \"/content/drive/MyDrive/ChromaDB_Parkinson_Data\"\n",
        "            os.makedirs(persist_dir, exist_ok=True)\n",
        "            return persist_dir\n",
        "\n",
        "        persist_dir = get_persistent_directory()\n",
        "        print(f\"✅ Google Drive mounted successfully!\")\n",
        "        print(f\"📁 Persistent directory set to: {persist_dir}\")\n",
        "\n",
        "        from IPython.display import display, HTML\n",
        "        notification = \"\"\"\n",
        "        <div style=\"background-color: #d1ecf1; border: 1px solid #bee5eb; color: #0c5460;\n",
        "                    padding: 15px; border-radius: 5px; margin: 10px 0; font-family: Arial;\">\n",
        "            <h3 style=\"margin-top: 0;\">☁️ Google Drive Integration Active!</h3>\n",
        "            <p>Your ChromaDB data will now be saved directly to Google Drive and automatically synced!</p>\n",
        "            <p><strong>📍 Location:</strong> Google Drive → ChromaDB_Parkinson_Data</p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(notification))\n",
        "\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(\"ℹ️  Google Drive mount not available (not in Colab environment)\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error mounting Google Drive: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# =============================================================================\n",
        "# 15. BACKUP AND RESTORE FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def backup_chromadb_to_zip():\n",
        "    \"\"\"\n",
        "    Create a ZIP backup of the ChromaDB persistent data\n",
        "    \"\"\"\n",
        "    import shutil\n",
        "\n",
        "    persist_dir = get_persistent_directory()\n",
        "    backup_path = os.path.join(os.path.dirname(persist_dir), \"ChromaDB_Backup.zip\")\n",
        "\n",
        "    try:\n",
        "        shutil.make_archive(backup_path.replace('.zip', ''), 'zip', persist_dir)\n",
        "\n",
        "        from IPython.display import display, HTML\n",
        "        notification = f\"\"\"\n",
        "        <div style=\"background-color: #d4edda; border: 1px solid #c3e6cb; color: #155724;\n",
        "                    padding: 15px; border-radius: 5px; margin: 10px 0; font-family: Arial;\">\n",
        "            <h3 style=\"margin-top: 0;\">📦 Backup Created Successfully!</h3>\n",
        "            <p><strong>📍 Backup Location:</strong> {backup_path}</p>\n",
        "            <p><strong>💡 Tip:</strong> You can now download or share this ZIP file containing your complete knowledge base!</p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(notification))\n",
        "\n",
        "        print(f\"✅ Backup created at: {backup_path}\")\n",
        "        return backup_path\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error creating backup: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def check_persistence_status():\n",
        "    \"\"\"\n",
        "    Check the status of persistent data\n",
        "    \"\"\"\n",
        "    persist_dir = get_persistent_directory()\n",
        "\n",
        "    print(\"🔍 PERSISTENCE STATUS CHECK\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"📁 Persistent Directory: {persist_dir}\")\n",
        "    print(f\"📂 Directory Exists: {os.path.exists(persist_dir)}\")\n",
        "\n",
        "    if os.path.exists(persist_dir):\n",
        "        files = os.listdir(persist_dir)\n",
        "        print(f\"📊 Files in directory: {len(files)}\")\n",
        "\n",
        "        if files:\n",
        "            print(\"📋 Contents:\")\n",
        "            for file in files[:10]:  # Show first 10 files\n",
        "                file_path = os.path.join(persist_dir, file)\n",
        "                size = os.path.getsize(file_path) if os.path.isfile(file_path) else \"DIR\"\n",
        "                print(f\"   - {file} ({size} bytes)\" if size != \"DIR\" else f\"   - {file}/ (directory)\")\n",
        "\n",
        "        # Check total size\n",
        "        total_size = 0\n",
        "        for dirpath, dirnames, filenames in os.walk(persist_dir):\n",
        "            for filename in filenames:\n",
        "                filepath = os.path.join(dirpath, filename)\n",
        "                total_size += os.path.getsize(filepath)\n",
        "\n",
        "        print(f\"💾 Total Size: {total_size / (1024*1024):.2f} MB\")\n",
        "    else:\n",
        "        print(\"❌ Persistent directory not found. Run the pipeline first!\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 16. EXECUTION ENTRY POINT - FIXED\n",
        "# =============================================================================\n",
        "\n",
        "def get_chroma_client(collection_name: str = \"parkinsons_knowledge_base\"):\n",
        "    \"\"\"\n",
        "    Initialize and return ChromaDB client with Google Embeddings and LOCAL PERSISTENCE.\n",
        "    \"\"\"\n",
        "    embedding_function = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "    # Get persistent directory\n",
        "    persist_dir = get_persistent_directory()\n",
        "\n",
        "    print(f\"📁 Using persistent directory: {persist_dir}\")\n",
        "\n",
        "    vector_store = Chroma(\n",
        "        collection_name=collection_name,\n",
        "        embedding_function=embedding_function,\n",
        "        persist_directory=persist_dir  # This makes it persistent!\n",
        "    )\n",
        "\n",
        "    return vector_store\n",
        "\n",
        "# Main execution block\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🚀 PERSISTENT CHROMADB RAG SYSTEM\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"💾 This version saves your data locally for persistence!\")\n",
        "    print(\"☁️  Optional: Run mount_google_drive_and_setup() for direct Google Drive storage\")\n",
        "    print()\n",
        "\n",
        "    # Option 1: Mount Google Drive first (recommended for Colab)\n",
        "    print(\"1️⃣  Mounting Google Drive (optional but recommended)...\")\n",
        "    try:\n",
        "        mount_success = mount_google_drive_and_setup()\n",
        "    except:\n",
        "        mount_success = False\n",
        "        print(\"ℹ️  Google Drive mount not available\")\n",
        "\n",
        "    # Option 2: Check current persistence status\n",
        "    print(\"\\n2️⃣  Checking persistence status...\")\n",
        "    try:\n",
        "        check_persistence_status()\n",
        "    except:\n",
        "        print(\"ℹ️  Will create persistence directory when needed\")\n",
        "\n",
        "    print(\"\\n3️⃣  Available commands:\")\n",
        "    print(\"   - run_complete_pipeline()                 # Full pipeline with persistence\")\n",
        "    print(\"   - process_complete_knowledge_base()       # Create knowledge base only\")\n",
        "    print(\"   - inspect_chromadb('collection_name')     # Inspect existing data\")\n",
        "    print(\"   - backup_chromadb_to_zip()               # Create ZIP backup\")\n",
        "    print(\"   - prepare_for_google_drive_upload()      # Upload instructions\")\n",
        "    print(\"   - check_persistence_status()             # Check data status\")\n",
        "\n",
        "    print(\"\\n✨ Ready to process your Parkinson's knowledge base with full persistence!\")\n",
        "\n",
        "    # Uncomment the line below to run the full pipeline automatically\n",
        "    run_complete_pipeline()\n",
        "\n",
        "# =============================================================================\n",
        "# 4. DATA LOADING AND FILE HANDLING - UNCHANGED\n",
        "# =============================================================================\n",
        "\n",
        "def upload_and_load_json():\n",
        "    \"\"\"\n",
        "    Upload JSON file using Google Colab file upload widget and load it.\n",
        "\n",
        "    Returns:\n",
        "        dict: Loaded data from JSON file\n",
        "    \"\"\"\n",
        "    from google.colab import files  # ✅ Add this import\n",
        "\n",
        "    print(\"Please upload your parkinsons_full_crawl.json file:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Get the uploaded file (should be the first and only file)\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    print(f\"✅ Uploaded file: {filename}\")\n",
        "\n",
        "    # Load and return the JSON data\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    print(f\"✅ Successfully loaded data with {len(data)} organizations\")\n",
        "    return data\n",
        "\n",
        "def load_crawled_data(json_file_path: str = None) -> Dict:\n",
        "    \"\"\"\n",
        "    Load the crawled Parkinson's data from JSON file.\n",
        "    If no path provided, will prompt for file upload.\n",
        "\n",
        "    Args:\n",
        "        json_file_path (str, optional): Path to the JSON file\n",
        "\n",
        "    Returns:\n",
        "        dict: Loaded data\n",
        "    \"\"\"\n",
        "    if json_file_path is None:\n",
        "        # Use file upload widget\n",
        "        return upload_and_load_json()\n",
        "    else:\n",
        "        # Try to load from specified path first\n",
        "        try:\n",
        "            with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            print(f\"✅ Successfully loaded data from {json_file_path}\")\n",
        "            return data\n",
        "        except FileNotFoundError:\n",
        "            print(f\"❌ File not found: {json_file_path}\")\n",
        "            print(\"Switching to file upload method...\")\n",
        "            return upload_and_load_json()\n",
        "\n",
        "# =============================================================================\n",
        "# 5. WEB PAGE CONTENT PROCESSING - UNCHANGED\n",
        "# =============================================================================\n",
        "\n",
        "def process_organization_data(org_name: str, org_data: Dict) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Process data from a single organization into document chunks.\n",
        "\n",
        "    Args:\n",
        "        org_name (str): Name of the organization\n",
        "        org_data (dict): Organization's crawled data\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: Processed document chunks\n",
        "    \"\"\"\n",
        "    all_docs = []\n",
        "\n",
        "    if 'pages_content' not in org_data:\n",
        "        print(f\"No pages content found for {org_name}\")\n",
        "        return all_docs\n",
        "\n",
        "    pages_content = org_data['pages_content']\n",
        "\n",
        "    for url, page_data in pages_content.items():\n",
        "        # Extract text and metadata\n",
        "        text = page_data.get('text', '')\n",
        "        page_metadata = page_data.get('metadata', {})\n",
        "\n",
        "        # Enhanced metadata\n",
        "        metadata = {\n",
        "            'organization': org_name,\n",
        "            'source_url': url,\n",
        "            'title': page_metadata.get('title', ''),\n",
        "            'description': page_metadata.get('description', ''),\n",
        "            'keywords': page_metadata.get('keywords', ''),\n",
        "            'crawl_depth': page_data.get('depth', 0),\n",
        "            'crawled_at': page_data.get('crawled_at', ''),\n",
        "            'base_url': org_data.get('base_url', ''),\n",
        "            'content_type': 'web_page'\n",
        "        }\n",
        "\n",
        "        # Process text into chunks\n",
        "        doc_chunks = get_doc_chunks(text, metadata)\n",
        "        all_docs.extend(doc_chunks)\n",
        "\n",
        "        if doc_chunks:\n",
        "            print(f\"Processed {len(doc_chunks)} chunks from {url}\")\n",
        "\n",
        "    return all_docs\n",
        "\n",
        "# =============================================================================\n",
        "# 6. MEDIA CONTENT PROCESSING - UNCHANGED\n",
        "# =============================================================================\n",
        "\n",
        "def process_media_content(org_name: str, org_data: Dict) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Process video and podcast content from organization data into document chunks.\n",
        "\n",
        "    Args:\n",
        "        org_name (str): Name of the organization\n",
        "        org_data (dict): Organization's crawled data\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: Media document chunks\n",
        "    \"\"\"\n",
        "    media_docs = []\n",
        "\n",
        "    if 'media_content' not in org_data:\n",
        "        return media_docs\n",
        "\n",
        "    media_content = org_data['media_content']\n",
        "\n",
        "    # Process videos\n",
        "    videos = media_content.get('videos', [])\n",
        "    for video in videos:\n",
        "        content_parts = []\n",
        "\n",
        "        if video.get('title'):\n",
        "            content_parts.append(f\"Video Title: {video['title']}\")\n",
        "\n",
        "        if video.get('description'):\n",
        "            content_parts.append(f\"Description: {video['description']}\")\n",
        "\n",
        "        content_parts.append(f\"Video URL: {video.get('url', 'N/A')}\")\n",
        "        content_parts.append(f\"Source Page: {video.get('source_page', 'N/A')}\")\n",
        "\n",
        "        content = \"\\n\".join(content_parts)\n",
        "\n",
        "        metadata = {\n",
        "            'organization': org_name,\n",
        "            'content_type': 'video',\n",
        "            'media_type': 'video',\n",
        "            'title': video.get('title', 'Untitled Video'),\n",
        "            'description': video.get('description', ''),\n",
        "            'media_url': video.get('url', ''),\n",
        "            'source_page': video.get('source_page', ''),\n",
        "            'base_url': org_data.get('base_url', ''),\n",
        "            'source_url': video.get('url', ''),\n",
        "        }\n",
        "\n",
        "        if len(content.strip()) > 50:\n",
        "            doc = Document(page_content=content, metadata=metadata)\n",
        "            media_docs.append(doc)\n",
        "\n",
        "    # Process podcasts\n",
        "    podcasts = media_content.get('podcasts', [])\n",
        "    for podcast in podcasts:\n",
        "        content_parts = []\n",
        "\n",
        "        if podcast.get('title'):\n",
        "            content_parts.append(f\"Podcast Title: {podcast['title']}\")\n",
        "\n",
        "        if podcast.get('description'):\n",
        "            content_parts.append(f\"Description: {podcast['description']}\")\n",
        "\n",
        "        content_parts.append(f\"Podcast URL: {podcast.get('url', 'N/A')}\")\n",
        "        content_parts.append(f\"Source Page: {podcast.get('source_page', 'N/A')}\")\n",
        "\n",
        "        content = \"\\n\".join(content_parts)\n",
        "\n",
        "        metadata = {\n",
        "            'organization': org_name,\n",
        "            'content_type': 'podcast',\n",
        "            'media_type': 'audio',\n",
        "            'title': podcast.get('title', 'Untitled Podcast'),\n",
        "            'description': podcast.get('description', ''),\n",
        "            'media_url': podcast.get('url', ''),\n",
        "            'source_page': podcast.get('source_page', ''),\n",
        "            'base_url': org_data.get('base_url', ''),\n",
        "            'source_url': podcast.get('url', ''),\n",
        "        }\n",
        "\n",
        "        if len(content.strip()) > 50:\n",
        "            doc = Document(page_content=content, metadata=metadata)\n",
        "            media_docs.append(doc)\n",
        "\n",
        "    return media_docs\n",
        "\n",
        "# =============================================================================\n",
        "# 7. BATCH STORAGE WITH RATE LIMITING - MODIFIED WITH PERSISTENCE NOTIFICATION\n",
        "# =============================================================================\n",
        "\n",
        "def store_documents_with_rate_limiting(docs: List[Document], vector_store: Chroma, batch_size: int = 25):\n",
        "    \"\"\"\n",
        "    Store documents in batches with rate limiting to avoid API quota issues.\n",
        "    Shows persistence notifications.\n",
        "\n",
        "    Args:\n",
        "        docs (List[Document]): Documents to store\n",
        "        vector_store (Chroma): ChromaDB vector store\n",
        "        batch_size (int): Number of documents per batch\n",
        "    \"\"\"\n",
        "    total_docs = len(docs)\n",
        "    print(f\"Storing {total_docs} documents in batches of {batch_size} with rate limiting\")\n",
        "\n",
        "    successful_batches = 0\n",
        "    failed_batches = 0\n",
        "\n",
        "    for i in range(0, total_docs, batch_size):\n",
        "        batch = docs[i:i + batch_size]\n",
        "        batch_num = i//batch_size + 1\n",
        "        total_batches = (total_docs + batch_size - 1)//batch_size\n",
        "\n",
        "        try:\n",
        "            print(f\"Processing batch {batch_num}/{total_batches}...\")\n",
        "            vector_store.add_documents(batch)\n",
        "            successful_batches += 1\n",
        "            print(f\"✅ Stored batch {batch_num}/{total_batches}\")\n",
        "\n",
        "            # Rate limiting: wait between batches\n",
        "            if batch_num < total_batches:\n",
        "                print(\"⏱️  Waiting 60 seconds to respect API rate limits...\")\n",
        "                time.sleep(60)\n",
        "\n",
        "        except Exception as e:\n",
        "            failed_batches += 1\n",
        "            print(f\"❌ Error storing batch {batch_num}: {str(e)}\")\n",
        "\n",
        "            # If it's a rate limit error, wait longer\n",
        "            if \"429\" in str(e) or \"RATE_LIMIT_EXCEEDED\" in str(e):\n",
        "                print(\"⏱️  Rate limit detected. Waiting 2 minutes before retrying...\")\n",
        "                time.sleep(120)\n",
        "\n",
        "                # Retry the failed batch once\n",
        "                try:\n",
        "                    print(f\"🔄 Retrying batch {batch_num}...\")\n",
        "                    vector_store.add_documents(batch)\n",
        "                    successful_batches += 1\n",
        "                    failed_batches -= 1\n",
        "                    print(f\"✅ Successfully stored batch {batch_num} on retry\")\n",
        "                except Exception as retry_error:\n",
        "                    print(f\"❌ Failed again on retry: {str(retry_error)}\")\n",
        "            continue\n",
        "\n",
        "    # PERSIST THE DATA AND SHOW NOTIFICATION\n",
        "    try:\n",
        "        vector_store.persist()\n",
        "        persist_dir = get_persistent_directory()\n",
        "        collection_name = vector_store._collection.name\n",
        "\n",
        "        print(f\"\\n📊 STORAGE SUMMARY:\")\n",
        "        print(f\"✅ Successful batches: {successful_batches}\")\n",
        "        print(f\"❌ Failed batches: {failed_batches}\")\n",
        "        print(f\"📄 Total documents attempted: {total_docs}\")\n",
        "        print(f\"📄 Estimated documents stored: {successful_batches * batch_size}\")\n",
        "        print(\"💾 All successful documents persisted to ChromaDB\")\n",
        "\n",
        "        # Show persistence notification\n",
        "        show_persistence_notification(persist_dir, collection_name, successful_batches * batch_size)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error persisting data: {str(e)}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 8. KNOWLEDGE BASE INSPECTION TOOLS - MODIFIED FOR PERSISTENCE\n",
        "# =============================================================================\n",
        "\n",
        "class ChromaDBInspector:\n",
        "    \"\"\"\n",
        "    Class to inspect and interact with your ChromaDB knowledge base\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, collection_name: str = \"parkinsons_knowledge_base\"):\n",
        "        self.collection_name = collection_name\n",
        "        self.embedding_function = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "        # Use persistent directory\n",
        "        persist_dir = get_persistent_directory()\n",
        "\n",
        "        self.vector_store = Chroma(\n",
        "            collection_name=collection_name,\n",
        "            embedding_function=self.embedding_function,\n",
        "            persist_directory=persist_dir  # Load from persistent directory\n",
        "        )\n",
        "\n",
        "    def get_collection_stats(self) -> Dict:\n",
        "        \"\"\"Get basic statistics about your ChromaDB collection\"\"\"\n",
        "        try:\n",
        "            collection = self.vector_store._collection\n",
        "            stats = {\n",
        "                \"collection_name\": collection.name,\n",
        "                \"total_documents\": collection.count(),\n",
        "                \"sample_ids\": list(collection.get()[\"ids\"][:5]) if collection.count() > 0 else [],\n",
        "                \"persistent_directory\": get_persistent_directory()\n",
        "            }\n",
        "            return stats\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def search_documents(self, query: str, k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Search for documents in your knowledge base\"\"\"\n",
        "        try:\n",
        "            results = self.vector_store.similarity_search_with_score(query, k=k)\n",
        "\n",
        "            formatted_results = []\n",
        "            for doc, score in results:\n",
        "                formatted_results.append({\n",
        "                    \"score\": score,\n",
        "                    \"organization\": doc.metadata.get(\"organization\", \"Unknown\"),\n",
        "                    \"title\": doc.metadata.get(\"title\", \"Unknown\"),\n",
        "                    \"url\": doc.metadata.get(\"source_url\", \"Unknown\"),\n",
        "                    \"content_preview\": doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content,\n",
        "                    \"content_type\": doc.metadata.get(\"content_type\", \"web_page\"),\n",
        "                    \"full_content\": doc.page_content,\n",
        "                    \"metadata\": doc.metadata\n",
        "                })\n",
        "\n",
        "            return formatted_results\n",
        "        except Exception as e:\n",
        "            return [{\"error\": str(e)}]\n",
        "\n",
        "    def get_all_organizations(self) -> List[str]:\n",
        "        \"\"\"Get list of all organizations in the database\"\"\"\n",
        "        try:\n",
        "            all_docs = self.vector_store.get()\n",
        "            organizations = set()\n",
        "\n",
        "            for metadata in all_docs[\"metadatas\"]:\n",
        "                org = metadata.get(\"organization\")\n",
        "                if org:\n",
        "                    organizations.add(org)\n",
        "\n",
        "            return sorted(list(organizations))\n",
        "        except Exception as e:\n",
        "            return [f\"Error: {str(e)}\"]\n",
        "\n",
        "    def export_sample_data(self, filename: str = \"chromadb_sample.json\", sample_size: int = 50):\n",
        "        \"\"\"Export a sample of your data to JSON for inspection\"\"\"\n",
        "        try:\n",
        "            all_docs = self.vector_store.get()\n",
        "            persist_dir = get_persistent_directory()\n",
        "\n",
        "            sample_data = {\n",
        "                \"total_documents\": len(all_docs[\"ids\"]),\n",
        "                \"sample_size\": min(sample_size, len(all_docs[\"ids\"])),\n",
        "                \"organizations\": self.get_all_organizations(),\n",
        "                \"persistent_directory\": persist_dir,\n",
        "                \"sample_documents\": []\n",
        "            }\n",
        "\n",
        "            for i in range(min(sample_size, len(all_docs[\"ids\"]))):\n",
        "                sample_data[\"sample_documents\"].append({\n",
        "                    \"id\": all_docs[\"ids\"][i],\n",
        "                    \"content_preview\": all_docs[\"documents\"][i][:200] + \"...\",\n",
        "                    \"metadata\": all_docs[\"metadatas\"][i]\n",
        "                })\n",
        "\n",
        "            # Save to persistent directory\n",
        "            export_path = os.path.join(persist_dir, filename)\n",
        "            with open(export_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(sample_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            print(f\"✅ Sample data exported to {export_path}\")\n",
        "            return sample_data\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error exporting data: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "# =============================================================================\n",
        "# 9. MEDIA SEARCH UTILITIES - UPDATED FOR PERSISTENCE\n",
        "# =============================================================================\n",
        "\n",
        "def search_media_content(query: str, collection_name: str = \"parkinsons_complete_kb\",\n",
        "                        media_type: str = \"all\", k: int = 5):\n",
        "    \"\"\"\n",
        "    Search specifically for media content\n",
        "\n",
        "    Args:\n",
        "        query (str): Search query\n",
        "        collection_name (str): ChromaDB collection name\n",
        "        media_type (str): \"all\", \"video\", or \"podcast\"\n",
        "        k (int): Number of results to return\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Formatted search results\n",
        "    \"\"\"\n",
        "    vector_store = get_chroma_client(collection_name)\n",
        "\n",
        "    # Get results\n",
        "    results = vector_store.similarity_search_with_score(query, k=k*3)\n",
        "\n",
        "    # Filter by media type\n",
        "    filtered_results = []\n",
        "    for doc, score in results:\n",
        "        content_type = doc.metadata.get('content_type', '')\n",
        "\n",
        "        if media_type == \"all\" and content_type in ['video', 'podcast']:\n",
        "            filtered_results.append((doc, score))\n",
        "        elif media_type == content_type:\n",
        "            filtered_results.append((doc, score))\n",
        "\n",
        "        if len(filtered_results) >= k:\n",
        "            break\n",
        "\n",
        "    # Format results\n",
        "    formatted_results = []\n",
        "    for doc, score in filtered_results:\n",
        "        formatted_results.append({\n",
        "            \"score\": score,\n",
        "            \"type\": doc.metadata.get('content_type', 'unknown'),\n",
        "            \"organization\": doc.metadata.get('organization', 'Unknown'),\n",
        "            \"title\": doc.metadata.get('title', 'Unknown'),\n",
        "            \"media_url\": doc.metadata.get('media_url', 'Unknown'),\n",
        "            \"description\": doc.metadata.get('description', ''),\n",
        "            \"content\": doc.page_content\n",
        "        })\n",
        "\n",
        "    return formatted_results\n",
        "\n",
        "# =============================================================================\n",
        "# 10. MAIN PROCESSING FUNCTIONS - MODIFIED WITH PERSISTENCE NOTIFICATIONS\n",
        "# =============================================================================\n",
        "\n",
        "def process_complete_knowledge_base(json_file_path: str = None,\n",
        "                                  collection_name: str = \"parkinsons_complete_kb\"):\n",
        "    \"\"\"\n",
        "    Process ALL content including text and media into a comprehensive knowledge base.\n",
        "    Now with LOCAL PERSISTENCE and notifications!\n",
        "\n",
        "    Args:\n",
        "        json_file_path (str, optional): Path to JSON data file\n",
        "        collection_name (str): Name for ChromaDB collection\n",
        "    \"\"\"\n",
        "    print(\"🚀 Starting COMPLETE Parkinson's Knowledge Base Creation (Text + Media + PERSISTENCE)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load the data\n",
        "    print(\"Loading crawled data...\")\n",
        "    try:\n",
        "        data = load_crawled_data(json_file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading data: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Initialize ChromaDB with persistence\n",
        "    print(\"Initializing ChromaDB with LOCAL PERSISTENCE...\")\n",
        "    try:\n",
        "        vector_store = get_chroma_client(collection_name)\n",
        "        persist_dir = get_persistent_directory()\n",
        "        print(f\"✅ ChromaDB initialized successfully - Collection: {collection_name}\")\n",
        "        print(f\"📁 Persistent directory: {persist_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error initializing ChromaDB: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    all_documents = []\n",
        "    media_documents = []\n",
        "\n",
        "    print(\"Processing organizations...\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for org_name, org_data in data.items():\n",
        "        if isinstance(org_data, dict):\n",
        "            print(f\"\\n📂 Processing {org_name}...\")\n",
        "\n",
        "            # Process text content\n",
        "            if 'pages_content' in org_data:\n",
        "                text_docs = process_organization_data(org_name, org_data)\n",
        "                all_documents.extend(text_docs)\n",
        "                print(f\"   📄 Generated {len(text_docs)} text document chunks\")\n",
        "\n",
        "            # Process media content\n",
        "            media_docs = process_media_content(org_name, org_data)\n",
        "            media_documents.extend(media_docs)\n",
        "            print(f\"   🎬 Generated {len(media_docs)} media document chunks\")\n",
        "\n",
        "        else:\n",
        "            print(f\"   ⚠️  Skipping {org_name} - no valid data\")\n",
        "\n",
        "    # Combine all documents\n",
        "    all_content = all_documents + media_documents\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"📊 COMPLETE SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"📄 Text documents: {len(all_documents)}\")\n",
        "    print(f\"🎬 Media documents: {len(media_documents)}\")\n",
        "    print(f\"📚 Total documents to store: {len(all_content)}\")\n",
        "\n",
        "    if all_content:\n",
        "        print(\"\\nStoring ALL documents in ChromaDB with rate limiting and PERSISTENCE...\")\n",
        "        try:\n",
        "            store_documents_with_rate_limiting(all_content, vector_store, batch_size=20)\n",
        "\n",
        "            print(f\"\\n🎉 SUCCESS! Your COMPLETE knowledge base is ready!\")\n",
        "            print(f\"   📚 Organizations processed: {len([k for k, v in data.items() if isinstance(v, dict)])}\")\n",
        "            print(f\"   📄 Text document chunks: {len(all_documents)}\")\n",
        "            print(f\"   🎬 Media document chunks: {len(media_documents)}\")\n",
        "            print(f\"   📊 Total document chunks: {len(all_content)}\")\n",
        "            print(f\"   🗄️  ChromaDB collection: {collection_name}\")\n",
        "            print(f\"   💾 Data persisted to: {get_persistent_directory()}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error storing documents: {str(e)}\")\n",
        "            return\n",
        "    else:\n",
        "        print(\"❌ No documents were generated. Check your data file.\")\n",
        "\n",
        "# =============================================================================\n",
        "# 11. UTILITY AND INSPECTION FUNCTIONS - UPDATED FOR PERSISTENCE\n",
        "# =============================================================================\n",
        "\n",
        "def inspect_chromadb(collection_name: str = \"parkinsons_knowledge_base\"):\n",
        "    \"\"\"\n",
        "    Inspect your ChromaDB knowledge base\n",
        "\n",
        "    Args:\n",
        "        collection_name (str): Name of the collection to inspect\n",
        "\n",
        "    Returns:\n",
        "        ChromaDBInspector: Inspector instance for further operations\n",
        "    \"\"\"\n",
        "    print(\"🔍 CHROMADB INSPECTOR (PERSISTENT VERSION)\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    inspector = ChromaDBInspector(collection_name)\n",
        "\n",
        "    # Get basic stats\n",
        "    print(\"📊 Collection Statistics:\")\n",
        "    stats = inspector.get_collection_stats()\n",
        "    for key, value in stats.items():\n",
        "        print(f\"   {key}: {value}\")\n",
        "\n",
        "    # Get organizations\n",
        "    print(\"\\n🏢 Organizations in Database:\")\n",
        "    orgs = inspector.get_all_organizations()\n",
        "    for i, org in enumerate(orgs, 1):\n",
        "        print(f\"   {i}. {org}\")\n",
        "\n",
        "    # Test search\n",
        "    print(\"\\n🔍 Sample Search Results:\")\n",
        "    test_queries = [\n",
        "        \"What are the symptoms of Parkinson's disease?\",\n",
        "        \"How to manage tremor?\",\n",
        "        \"exercise therapy\"\n",
        "    ]\n",
        "\n",
        "    for query in test_queries:\n",
        "        print(f\"\\nQuery: '{query}'\")\n",
        "        results = inspector.search_documents(query, k=2)\n",
        "        for i, result in enumerate(results[:2], 1):\n",
        "            if \"error\" not in result:\n",
        "                print(f\"   {i}. {result['organization']} - Score: {result['score']:.3f}\")\n",
        "                print(f\"      Type: {result.get('content_type', 'text')}\")\n",
        "                print(f\"      {result['content_preview']}\")\n",
        "\n",
        "    return inspector\n",
        "\n",
        "# =============================================================================\n",
        "# 12. GOOGLE DRIVE UPLOAD HELPER\n",
        "# =============================================================================\n",
        "\n",
        "def prepare_for_google_drive_upload():\n",
        "    \"\"\"\n",
        "    Show instructions for uploading persistent data to Google Drive\n",
        "    \"\"\"\n",
        "    persist_dir = get_persistent_directory()\n",
        "\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    instructions = f\"\"\"\n",
        "    <div style=\"background-color: #e7f3ff; border: 1px solid #b8daff; color: #004085;\n",
        "                padding: 20px; border-radius: 5px; margin: 10px 0; font-family: Arial;\">\n",
        "        <h3 style=\"margin-top: 0;\">📤 Ready for Google Drive Upload!</h3>\n",
        "\n",
        "        <h4>📁 Your persistent data location:</h4>\n",
        "        <code style=\"background-color: #f8f9fa; padding: 5px; border-radius: 3px;\">{persist_dir}</code>\n",
        "\n",
        "        <h4>🚀 Upload Steps:</h4>\n",
        "        <ol>\n",
        "            <li><strong>Compress the folder:</strong> Right-click on the ChromaDB_Parkinson_Data folder and create a ZIP file</li>\n",
        "            <li><strong>Upload to Google Drive:</strong> Upload the ZIP file to your Google Drive</li>\n",
        "            <li><strong>Share with team:</strong> Share the folder with team members if needed</li>\n",
        "        </ol>\n",
        "\n",
        "        <h4>💡 Alternative - Direct Google Drive mount (in Colab):</h4>\n",
        "        <p>If you're in Google Colab, you can mount Google Drive and save directly there!</p>\n",
        "\n",
        "        <h4>📋 What's included in your persistent data:</h4>\n",
        "        <ul>\n",
        "            <li>🗄️ Complete ChromaDB vector database</li>\n",
        "            <li>📊 All document embeddings</li>\n",
        "            <li>🔍 Searchable knowledge base</li>\n",
        "            <li>📁 Collection metadata</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(instructions))\n",
        "    print(f\"📁 Persistent data ready at: {persist_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lwBXIP_GMiHP",
        "outputId": "42b5751a-c0e2-4c39-b685-6e6e352df0b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ API key loaded from Colab secrets\n",
            "🚀 PERSISTENT CHROMADB RAG SYSTEM\n",
            "==================================================\n",
            "💾 This version saves your data locally for persistence!\n",
            "☁️  Optional: Run mount_google_drive_and_setup() for direct Google Drive storage\n",
            "\n",
            "1️⃣  Mounting Google Drive (optional but recommended)...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Google Drive mounted successfully!\n",
            "📁 Persistent directory set to: /content/drive/MyDrive/ChromaDB_Parkinson_Data\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <div style=\"background-color: #d1ecf1; border: 1px solid #bee5eb; color: #0c5460;\n",
              "                    padding: 15px; border-radius: 5px; margin: 10px 0; font-family: Arial;\">\n",
              "            <h3 style=\"margin-top: 0;\">☁️ Google Drive Integration Active!</h3>\n",
              "            <p>Your ChromaDB data will now be saved directly to Google Drive and automatically synced!</p>\n",
              "            <p><strong>📍 Location:</strong> Google Drive → ChromaDB_Parkinson_Data</p>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2️⃣  Checking persistence status...\n",
            "🔍 PERSISTENCE STATUS CHECK\n",
            "========================================\n",
            "📁 Persistent Directory: /content/drive/MyDrive/ChromaDB_Parkinson_Data\n",
            "📂 Directory Exists: True\n",
            "📊 Files in directory: 2\n",
            "📋 Contents:\n",
            "   - chroma.sqlite3 (163840 bytes)\n",
            "   - complete_knowledge_base_sample.json (171 bytes)\n",
            "💾 Total Size: 0.16 MB\n",
            "\n",
            "3️⃣  Available commands:\n",
            "   - run_complete_pipeline()                 # Full pipeline with persistence\n",
            "   - process_complete_knowledge_base()       # Create knowledge base only\n",
            "   - inspect_chromadb('collection_name')     # Inspect existing data\n",
            "   - backup_chromadb_to_zip()               # Create ZIP backup\n",
            "   - prepare_for_google_drive_upload()      # Upload instructions\n",
            "   - check_persistence_status()             # Check data status\n",
            "\n",
            "✨ Ready to process your Parkinson's knowledge base with full persistence!\n",
            "🚀 STARTING COMPLETE PARKINSON'S RAG PIPELINE WITH PERSISTENCE\n",
            "============================================================\n",
            "\n",
            "📚 STEP 1: Creating complete knowledge base with LOCAL PERSISTENCE...\n",
            "🚀 Starting COMPLETE Parkinson's Knowledge Base Creation (Text + Media + PERSISTENCE)\n",
            "======================================================================\n",
            "Loading crawled data...\n",
            "Please upload your parkinsons_full_crawl.json file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-15f09b85-ba25-4e97-a234-f981dacc0071\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-15f09b85-ba25-4e97-a234-f981dacc0071\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving parkinsons_full_crawl.json to parkinsons_full_crawl.json\n",
            "✅ Uploaded file: parkinsons_full_crawl.json\n",
            "✅ Successfully loaded data with 9 organizations\n",
            "Initializing ChromaDB with LOCAL PERSISTENCE...\n",
            "📁 Using persistent directory: /content/drive/MyDrive/ChromaDB_Parkinson_Data\n",
            "✅ ChromaDB initialized successfully - Collection: parkinsons_complete_kb\n",
            "📁 Persistent directory: /content/drive/MyDrive/ChromaDB_Parkinson_Data\n",
            "Processing organizations...\n",
            "----------------------------------------\n",
            "\n",
            "📂 Processing Parkinson's Foundation...\n",
            "Processed 12 chunks from https://www.parkinson.org/\n",
            "Processed 15 chunks from https://www.parkinson.org/understanding-parkinsons/movement-symptoms\n",
            "Processed 27 chunks from https://www.parkinson.org/advancing-research/advocate-research\n",
            "Processed 10 chunks from https://www.parkinson.org/living-with-parkinsons/stories\n",
            "Processed 15 chunks from https://www.parkinson.org/understanding-parkinsons/10-early-signs\n",
            "Processed 13 chunks from https://www.parkinson.org/resources-support/carepartners\n",
            "Processed 15 chunks from https://www.parkinson.org/about-us/careers\n",
            "Processed 18 chunks from https://www.parkinson.org/advancing-research/our-research/pdgeneration\n",
            "Processed 11 chunks from https://www.parkinson.org/living-with-parkinsons/finding-care\n",
            "Processed 12 chunks from https://www.parkinson.org/how-to-help/tribute\n",
            "Processed 11 chunks from https://www.parkinson.org/living-with-parkinsons/management\n",
            "Processed 10 chunks from https://www.parkinson.org/resources-support/online-education\n",
            "Processed 10 chunks from https://www.parkinson.org/about-us/contact\n",
            "Processed 22 chunks from https://www.parkinson.org/advancing-research/parkinsons-virtual-biotech\n",
            "Processed 12 chunks from https://www.parkinson.org/resources-support/events\n",
            "Processed 12 chunks from https://www.parkinson.org/resources-support/blog\n",
            "Processed 13 chunks from https://www.parkinson.org/about-us/news/new-chief-medical-officer\n",
            "Processed 18 chunks from https://www.parkinson.org/understanding-parkinsons/getting-diagnosed\n",
            "Processed 12 chunks from https://www.parkinson.org/understanding-parkinsons\n",
            "Processed 11 chunks from https://www.parkinson.org/resources-support/professionals\n",
            "Processed 8 chunks from https://www.parkinson.org/resources-support/pd-library/order-publications\n",
            "Processed 12 chunks from https://www.parkinson.org/understanding-parkinsons/causes\n",
            "Processed 14 chunks from https://www.parkinson.org/living-with-parkinsons/care-programs\n",
            "Processed 11 chunks from https://www.parkinson.org/events/2025/SEAmbassadors\n",
            "Processed 11 chunks from https://www.parkinson.org/how-to-help\n",
            "Processed 11 chunks from https://www.parkinson.org/events/2025/July18FF\n",
            "Processed 12 chunks from https://www.parkinson.org/how-to-help/fundraise\n",
            "Processed 12 chunks from https://www.parkinson.org/advancing-research/for-researchers\n",
            "Processed 11 chunks from https://www.parkinson.org/events/2025/July16WW\n",
            "Processed 13 chunks from https://www.parkinson.org/advancing-research/finding-cure\n",
            "Processed 12 chunks from https://www.parkinson.org/resources-support\n",
            "Processed 11 chunks from https://www.parkinson.org/events/2025/July14MM\n",
            "Processed 11 chunks from https://www.parkinson.org/advancing-research\n",
            "Processed 11 chunks from https://www.parkinson.org/living-with-parkinsons\n",
            "Processed 14 chunks from https://www.parkinson.org/how-to-help/volunteer\n",
            "Processed 12 chunks from https://www.parkinson.org\n",
            "Processed 9 chunks from https://www.parkinson.org/espanol\n",
            "Processed 16 chunks from https://www.parkinson.org/resources-support/hospital-safety-guide\n",
            "Processed 14 chunks from https://www.parkinson.org/how-to-help/why-support-us\n",
            "Processed 16 chunks from https://www.parkinson.org/blog/awareness/sleep-disorders-expert-briefing\n",
            "Processed 10 chunks from https://www.parkinson.org/resources-support/find-your-chapter\n",
            "Processed 12 chunks from https://www.parkinson.org/living-with-parkinsons/legal-financial\n",
            "Processed 10 chunks from https://www.parkinson.org/about-us\n",
            "Processed 20 chunks from https://www.parkinson.org/resources-support/community-grants\n",
            "Processed 10 chunks from https://www.parkinson.org/resources-support/pd-library\n",
            "Processed 12 chunks from https://www.parkinson.org/living-with-parkinsons/stories/jack-ryan\n",
            "Processed 20 chunks from https://www.parkinson.org/understanding-parkinsons/non-movement-symptoms\n",
            "Processed 12 chunks from https://www.parkinson.org/about-us/press-room\n",
            "Processed 17 chunks from https://www.parkinson.org/understanding-parkinsons/statistics\n",
            "Processed 11 chunks from https://www.parkinson.org/blog/advancing-research/early-onset-parkinsons\n",
            "Processed 14 chunks from https://www.parkinson.org/living-with-parkinsons/emotional-mental-health\n",
            "Processed 13 chunks from https://www.parkinson.org/living-with-parkinsons/treatment\n",
            "Processed 12 chunks from https://www.parkinson.org/advancing-research/our-research\n",
            "Processed 14 chunks from https://www.parkinson.org/resources-support/helpline\n",
            "Processed 12 chunks from https://www.parkinson.org/how-to-help/more-ways-to-give\n",
            "Processed 14 chunks from https://www.parkinson.org/blog/awareness/2025-survey\n",
            "Processed 19 chunks from https://www.parkinson.org/privacy\n",
            "Processed 13 chunks from https://www.parkinson.org/how-to-help/corporate-sponsors\n",
            "Processed 13 chunks from https://www.parkinson.org/living-with-parkinsons/new-to-parkinsons\n",
            "Processed 13 chunks from https://www.parkinson.org/advancing-research/join-study\n",
            "Processed 9 chunks from https://www.parkinson.org/your-area\n",
            "Processed 13 chunks from https://www.parkinson.org/understanding-parkinsons/what-is-parkinsons\n",
            "Processed 15 chunks from https://www.parkinson.org/library/fact-sheets/occupational-therapy\n",
            "Processed 18 chunks from https://www.parkinson.org/understanding-parkinsons/non-movement-symptoms/pain\n",
            "Processed 13 chunks from https://www.parkinson.org/understanding-parkinsons/non-movement-symptoms/skin\n",
            "Processed 12 chunks from https://www.parkinson.org/understanding-parkinsons/movement-symptoms/small-handwriting\n",
            "Processed 19 chunks from https://www.parkinson.org/blog/awareness/nourishing-wellness\n",
            "Processed 14 chunks from https://www.parkinson.org/understanding-parkinsons/what-is-parkinsons/myths\n",
            "Processed 15 chunks from https://www.parkinson.org/understanding-parkinsons/getting-diagnosed/conditions-that-mimic-parkinsons\n",
            "Processed 14 chunks from https://www.parkinson.org/understanding-parkinsons/movement-symptoms/drooling\n",
            "Processed 13 chunks from https://www.parkinson.org/understanding-parkinsons/non-movement-symptoms/vision\n",
            "Processed 15 chunks from https://www.parkinson.org/understanding-parkinsons/non-movement-symptoms/weight-management\n",
            "Processed 13 chunks from https://www.parkinson.org/understanding-parkinsons/non-movement-symptoms/apathy\n",
            "Processed 12 chunks from https://www.parkinson.org/understanding-parkinsons/movement-symptoms/stooped-posture\n",
            "Processed 12 chunks from https://www.parkinson.org/advancing-research/our-research/parkinsons-outcomes-project\n",
            "Processed 13 chunks from https://www.parkinson.org/understanding-parkinsons/movement-symptoms/tremor\n",
            "Processed 16 chunks from https://www.parkinson.org/understanding-parkinsons/movement-symptoms/dystonia\n",
            "Processed 16 chunks from https://www.parkinson.org/library/fact-sheets/10-signs\n",
            "Processed 16 chunks from https://www.parkinson.org/understanding-parkinsons/getting-diagnosed/biomarkers\n",
            "Processed 16 chunks from https://www.parkinson.org/understanding-parkinsons/causes/genetics\n",
            "Processed 11 chunks from https://www.parkinson.org/resources-support/online-education/pdhealth\n",
            "Processed 14 chunks from https://www.parkinson.org/library/fact-sheets/managing-off-time\n",
            "Processed 11 chunks from https://www.parkinson.org/understanding-parkinsons/movement-symptoms/rigidity\n",
            "Processed 15 chunks from https://www.parkinson.org/blog/awareness/what-professionals-should-know\n",
            "Processed 17 chunks from https://www.parkinson.org/understanding-parkinsons/what-is-parkinsons/stages\n",
            "Processed 14 chunks from https://www.parkinson.org/understanding-parkinsons/what-is-parkinsons/related-conditions\n",
            "Processed 24 chunks from https://www.parkinson.org/understanding-parkinsons/non-movement-symptoms/cognitive\n",
            "Processed 12 chunks from https://www.parkinson.org/understanding-parkinsons/movement-symptoms/facial-masking\n",
            "Processed 21 chunks from https://www.parkinson.org/understanding-parkinsons/non-movement-symptoms/incontinence\n",
            "Processed 17 chunks from https://www.parkinson.org/understanding-parkinsons/what-is-parkinsons/types-parkinsonisms\n",
            "Processed 13 chunks from https://www.parkinson.org/understanding-parkinsons/movement-symptoms/postural-instability\n",
            "Processed 13 chunks from https://www.parkinson.org/understanding-parkinsons/non-movement-symptoms/fatigue\n",
            "Processed 11 chunks from https://www.parkinson.org/understanding-parkinsons/movement-symptoms/bradykinesia\n",
            "Processed 16 chunks from https://www.parkinson.org/understanding-parkinsons/non-movement-symptoms/bone-health\n",
            "Processed 14 chunks from https://www.parkinson.org/library/fact-sheets/pain\n",
            "Processed 15 chunks from https://www.parkinson.org/understanding-parkinsons/non-movement-symptoms/constipation\n",
            "Processed 15 chunks from https://www.parkinson.org/understanding-parkinsons/statistics/prevalence-incidence\n",
            "Processed 23 chunks from https://www.parkinson.org/understanding-parkinsons/non-movement-symptoms/hallucinations-delusions\n",
            "Processed 10 chunks from https://www.parkinson.org/blog/awareness/top-pam-videos\n",
            "   📄 Generated 1361 text document chunks\n",
            "   🎬 Generated 94 media document chunks\n",
            "\n",
            "📂 Processing Michael J. Fox Foundation...\n",
            "Processed 10 chunks from https://www.michaeljfox.org/\n",
            "Processed 13 chunks from https://www.michaeljfox.org/news/ask-md-next-generation-dbs-here\n",
            "Processed 12 chunks from https://www.michaeljfox.org/our-impact\n",
            "Processed 9 chunks from https://www.michaeljfox.org/news/michael-j-fox-announces-new-memoir-future-boy-be-published-fall\n",
            "Processed 12 chunks from https://www.michaeljfox.org/symptoms\n",
            "Processed 14 chunks from https://www.michaeljfox.org/conociendo-la-enfermedad-de-parkinson\n",
            "Processed 9 chunks from https://www.michaeljfox.org/contact-your-policymakers\n",
            "Processed 20 chunks from https://www.michaeljfox.org/scientific-publications\n",
            "Processed 11 chunks from https://www.michaeljfox.org/your-role-parkinsons-research\n",
            "Processed 9 chunks from https://www.michaeljfox.org/working-us\n",
            "Processed 11 chunks from https://www.michaeljfox.org/parkinsons-360\n",
            "Processed 11 chunks from https://www.michaeljfox.org/funded-studies\n",
            "Processed 11 chunks from https://www.michaeljfox.org/be-fundraiser\n",
            "Processed 10 chunks from https://www.michaeljfox.org/user/logout\n",
            "Processed 46 chunks from https://www.michaeljfox.org/our-public-policy-priorities\n",
            "Processed 13 chunks from https://www.michaeljfox.org/do-it-yourself\n",
            "Processed 9 chunks from https://www.michaeljfox.org/research-news\n",
            "Processed 9 chunks from https://www.michaeljfox.org/mental-physical-health\n",
            "Processed 16 chunks from https://www.michaeljfox.org/funding-opportunities\n",
            "Processed 13 chunks from https://www.michaeljfox.org/news/what-we-fund-606m-tools-detect-pd-and-therapies-treat-it\n",
            "Processed 45 chunks from https://www.michaeljfox.org/glossary-terms\n",
            "Processed 48 chunks from https://www.michaeljfox.org/faq\n",
            "Processed 10 chunks from https://www.michaeljfox.org/webinar/finding-right-tech-tools-your-parkinsons-journey\n",
            "Processed 13 chunks from https://www.michaeljfox.org/current-partnerships\n",
            "Processed 10 chunks from https://www.michaeljfox.org/ask-md\n",
            "Processed 9 chunks from https://www.michaeljfox.org/real-talk-patients\n",
            "Processed 9 chunks from https://www.michaeljfox.org/podcast/building-your-best-parkinsons-care-team-webinar-audio\n",
            "Processed 10 chunks from https://www.michaeljfox.org/podcast\n",
            "Processed 11 chunks from https://www.michaeljfox.org/news/drug-potential-slow-or-stop-parkinsons-moves-next-phase-clinical-testing\n",
            "Processed 7 chunks from https://www.michaeljfox.org/work-benefits\n",
            "Processed 10 chunks from https://www.michaeljfox.org/our-agenda\n",
            "Processed 10 chunks from https://www.michaeljfox.org/webinars\n",
            "Processed 12 chunks from https://www.michaeljfox.org/national-parkinsons-project\n",
            "Processed 10 chunks from https://www.michaeljfox.org/news/watch-jimmy-choi-inspire-millions-and-achieve-new-personal-record-american-ninja-warrior\n",
            "Processed 13 chunks from https://www.michaeljfox.org/give-tribute\n",
            "Processed 7 chunks from https://www.michaeljfox.org/join-study\n",
            "Processed 8 chunks from https://www.michaeljfox.org/related-conditions\n",
            "Processed 14 chunks from https://www.michaeljfox.org/our-research-strategy\n",
            "Processed 9 chunks from https://www.michaeljfox.org/prizes\n",
            "Processed 8 chunks from https://www.michaeljfox.org/parkinsons-iq-you\n",
            "Processed 7 chunks from https://www.michaeljfox.org/build-connections-parkinsons-buddy-network\n",
            "Processed 8 chunks from https://www.michaeljfox.org/contact-us\n",
            "Processed 63 chunks from https://www.michaeljfox.org/study-recruitment\n",
            "Processed 13 chunks from https://www.michaeljfox.org/corporate-giving\n",
            "Processed 11 chunks from https://www.michaeljfox.org/news/what-we-fund-497m-supports-parkinsons-biology-insights-and-more\n",
            "Processed 20 chunks from https://www.michaeljfox.org/privacy-policy\n",
            "Processed 7 chunks from https://www.michaeljfox.org/past-annual-reports\n",
            "Processed 13 chunks from https://www.michaeljfox.org/parkinsons-biomarkers\n",
            "Processed 11 chunks from https://www.michaeljfox.org/ways-to-give\n",
            "Processed 9 chunks from https://www.michaeljfox.org/real-talk-participants\n",
            "Processed 9 chunks from https://www.michaeljfox.org/foundation-leadership\n",
            "Processed 9 chunks from https://www.michaeljfox.org/find-fundraiser-or-event\n",
            "Processed 13 chunks from https://www.michaeljfox.org/news/advancing-parkinsons-research-through-patient-centered-approach\n",
            "Processed 11 chunks from https://www.michaeljfox.org/webinar/plan-purpose-your-estate-plan-after-parkinsons-diagnosis\n",
            "Processed 14 chunks from https://www.michaeljfox.org/state-field\n",
            "Processed 10 chunks from https://www.michaeljfox.org/press-releases\n",
            "Processed 10 chunks from https://www.michaeljfox.org/mjff-news\n",
            "Processed 11 chunks from https://www.michaeljfox.org/data-resources\n",
            "Processed 25 chunks from https://www.michaeljfox.org/2019-2020-annual-report-milestones-and-momentum\n",
            "Processed 11 chunks from https://www.michaeljfox.org/news/tracers-development-newly-installed-pet-camera-signal-advancement-parkinsons-imaging\n",
            "Processed 10 chunks from https://www.michaeljfox.org/research-tools\n",
            "Processed 1 chunks from https://www.michaeljfox.org/sitemap.xml\n",
            "Processed 13 chunks from https://www.michaeljfox.org/michaels-story\n",
            "Processed 8 chunks from https://www.michaeljfox.org/mjff-feed\n",
            "Processed 8 chunks from https://www.michaeljfox.org/team-fox-endurance\n",
            "Processed 11 chunks from https://www.michaeljfox.org/medications-treatments\n",
            "Processed 10 chunks from https://www.michaeljfox.org/causes\n",
            "Processed 12 chunks from https://www.michaeljfox.org/books-resources\n",
            "Processed 8 chunks from https://www.michaeljfox.org/publications\n",
            "Processed 26 chunks from https://www.michaeljfox.org/biospecimens\n",
            "Processed 9 chunks from https://www.michaeljfox.org/updates-washington\n",
            "Processed 9 chunks from https://www.michaeljfox.org/advocacy-resources\n",
            "Processed 17 chunks from https://www.michaeljfox.org/asap\n",
            "Processed 20 chunks from https://www.michaeljfox.org/key-research-initiatives\n",
            "Processed 10 chunks from https://www.michaeljfox.org/our-commitment-research-integrity\n",
            "Processed 19 chunks from https://www.michaeljfox.org/ppmi\n",
            "Processed 11 chunks from https://www.michaeljfox.org/what-we-fund\n",
            "Processed 7 chunks from https://www.michaeljfox.org/trial-finder\n",
            "Processed 10 chunks from https://www.michaeljfox.org/resources-people-newly-diagnosed-parkinsons\n",
            "Processed 9 chunks from https://www.michaeljfox.org/building-care-team\n",
            "Processed 9 chunks from https://www.michaeljfox.org/podcast/understanding-experience-lgbtq-people-living-parkinsons-disease\n",
            "Processed 7 chunks from https://www.michaeljfox.org/relationships\n",
            "Processed 20 chunks from https://www.michaeljfox.org/collaborative-research-opportunities\n",
            "Processed 10 chunks from https://www.michaeljfox.org/ppmi-clinical-study\n",
            "Processed 11 chunks from https://www.michaeljfox.org/state-nonprofit-disclosures\n",
            "Processed 9 chunks from https://www.michaeljfox.org/news/leading-parkinsons-advocacy-groups-urge-strong-federal-investment-biomedical-research\n",
            "Processed 17 chunks from https://www.michaeljfox.org/terms-conditions\n",
            "Processed 16 chunks from https://www.michaeljfox.org/news/early-onset-parkinsons-disease\n",
            "Processed 12 chunks from https://www.michaeljfox.org/our-promise\n",
            "Processed 9 chunks from https://www.michaeljfox.org/news/runwalk-and-cycling-series\n",
            "Processed 12 chunks from https://www.michaeljfox.org/careers\n",
            "Processed 30 chunks from https://www.michaeljfox.org/parkinsons-101\n",
            "Processed 13 chunks from https://www.michaeljfox.org/fundraiser-resources\n",
            "Processed 8 chunks from https://www.michaeljfox.org/news/parkinsons-same-parkinsonism-ask-md-video\n",
            "Processed 9 chunks from https://www.michaeljfox.org/foundation-updates\n",
            "Processed 11 chunks from https://www.michaeljfox.org/news/research-roundup-join-studies-aiming-treat-parkinsons-progression\n",
            "   📄 Generated 1257 text document chunks\n",
            "   🎬 Generated 85 media document chunks\n",
            "\n",
            "📂 Processing American Parkinson Disease Association...\n",
            "Processed 8 chunks from https://www.apdaparkinson.org/\n",
            "Processed 8 chunks from https://www.apdaparkinson.org/about-apda\n",
            "Processed 9 chunks from https://www.apdaparkinson.org/living-with-parkinsons-disease\n",
            "Processed 7 chunks from https://www.apdaparkinson.org/news\n",
            "Processed 9 chunks from https://www.apdaparkinson.org/what-is-parkinsons\n",
            "Processed 11 chunks from https://www.apdaparkinson.org/living-with-parkinsons-disease/relationships\n",
            "Processed 10 chunks from https://www.apdaparkinson.org/doctor-blogs/a-closer-look\n",
            "Processed 8 chunks from https://www.apdaparkinson.org/events/thriving-through-occupational-therapy-living-well-with-parkinsons-fatigue\n",
            "Processed 8 chunks from https://www.apdaparkinson.org/research/advanced-centers\n",
            "Processed 13 chunks from https://www.apdaparkinson.org/living-with-parkinsons-disease/treatment-medication\n",
            "Processed 7 chunks from https://www.apdaparkinson.org/research\n",
            "Processed 14 chunks from https://www.apdaparkinson.org/what-is-parkinsons/symptoms\n",
            "Processed 5 chunks from https://www.apdaparkinson.org/get-involved/memorial-and-tribute-gifts\n",
            "Processed 10 chunks from https://www.apdaparkinson.org/article/neuromodulation-for-parkinsons-disease\n",
            "Processed 6 chunks from https://www.apdaparkinson.org/resources-support/rehab-resource-center\n",
            "Processed 12 chunks from https://www.apdaparkinson.org/article/nanocarrier-breakthrough-offers-hope-for-pd-treatment\n",
            "Processed 7 chunks from https://www.apdaparkinson.org/disclosure-statement\n",
            "Processed 40 chunks from https://www.apdaparkinson.org/resources\n",
            "Processed 10 chunks from https://www.apdaparkinson.org/research/research-opportunities\n",
            "Processed 10 chunks from https://www.apdaparkinson.org/research/what-we-fund\n",
            "Processed 58 chunks from https://www.apdaparkinson.org/upcoming-events\n",
            "Processed 8 chunks from https://www.apdaparkinson.org/resources-support/en-espanol\n",
            "Processed 13 chunks from https://www.apdaparkinson.org/1907d3l\n",
            "Processed 10 chunks from https://www.apdaparkinson.org/what-is-parkinsons/diagnosing\n",
            "Processed 11 chunks from https://www.apdaparkinson.org/resources-support\n",
            "Processed 17 chunks from https://www.apdaparkinson.org/research/clinical-trials\n",
            "Processed 8 chunks from https://www.apdaparkinson.org/resources-support/download-publications\n",
            "Processed 6 chunks from https://www.apdaparkinson.org/about-apda/financial-reports\n",
            "Processed 10 chunks from https://www.apdaparkinson.org/events/dr-gilbert-hosts-climate-weather-parkinsons-disease\n",
            "Processed 6 chunks from https://www.apdaparkinson.org/professional-training-courses\n",
            "Processed 5 chunks from https://www.apdaparkinson.org/about-apda/strategic-plan\n",
            "Processed 7 chunks from https://www.apdaparkinson.org/what-is-parkinsons/causes\n",
            "Processed 6 chunks from https://www.apdaparkinson.org/champion\n",
            "Processed 10 chunks from https://www.apdaparkinson.org/what-is-parkinsons/early-onset-parkinsons-disease\n",
            "Processed 8 chunks from https://www.apdaparkinson.org/community\n",
            "Processed 7 chunks from http://www.apdaparkinson.org/research\n",
            "Processed 5 chunks from https://www.apdaparkinson.org/get-involved/optimism-walks\n",
            "Processed 7 chunks from https://www.apdaparkinson.org/terms-of-use\n",
            "Processed 12 chunks from https://www.apdaparkinson.org/living-with-parkinsons-disease/diet-nutrition\n",
            "Processed 5 chunks from http://www.apdaparkinson.org/get-involved/ways-to-give\n",
            "Processed 9 chunks from https://www.apdaparkinson.org/donor-policy\n",
            "Processed 14 chunks from https://www.apdaparkinson.org/living-with-parkinsons-disease/mental-health\n",
            "Processed 9 chunks from http://www.apdaparkinson.org/what-is-parkinsons\n",
            "Processed 7 chunks from https://www.apdaparkinson.org/about-apda/career-opportunities\n",
            "Processed 12 chunks from https://www.apdaparkinson.org/living-with-parkinsons-disease/exercise\n",
            "Processed 7 chunks from https://www.apdaparkinson.org/resources-support/for-caregivers\n",
            "Processed 5 chunks from https://www.apdaparkinson.org/apda-symptom-tracker\n",
            "Processed 5 chunks from https://www.apdaparkinson.org/get-involved/diy-fundraising\n",
            "Processed 5 chunks from https://www.apdaparkinson.org/get-involved/ways-to-give\n",
            "Processed 6 chunks from https://www.apdaparkinson.org/newly-diagnosed\n",
            "Processed 31 chunks from https://www.apdaparkinson.org/privacy-policy\n",
            "Processed 14 chunks from https://www.apdaparkinson.org/living-with-parkinsons-disease/treatment-medication/medication\n",
            "Processed 10 chunks from https://www.apdaparkinson.org/article/a-new-blood-test-could-identify-parkinsons-disease-before-symptoms-appear\n",
            "Processed 6 chunks from https://www.apdaparkinson.org/contact\n",
            "Processed 8 chunks from https://www.apdaparkinson.org\n",
            "Processed 7 chunks from https://www.apdaparkinson.org/living-with-parkinsons-disease/disability\n",
            "Processed 16 chunks from https://www.apdaparkinson.org/article/parkinsons-research-insights-dr-greenamyre\n",
            "Processed 8 chunks from http://www.apdaparkinson.org\n",
            "Processed 6 chunks from https://www.apdaparkinson.org/about-apda/eloise-caggiano\n",
            "Processed 6 chunks from https://www.apdaparkinson.org/about-apda/rosa-pena\n",
            "Processed 5 chunks from https://www.apdaparkinson.org/about-apda/chris-salicco\n",
            "Processed 5 chunks from https://www.apdaparkinson.org/core-values\n",
            "Processed 7 chunks from https://www.apdaparkinson.org/about-apda/scientific-advisory-board\n",
            "Processed 5 chunks from https://www.apdaparkinson.org/about-apda/heather-hays\n",
            "Processed 6 chunks from https://www.apdaparkinson.org/about-apda/leslie-a-chambers\n",
            "Processed 6 chunks from https://www.apdaparkinson.org/about-apda/michelle-h-mcdonald\n",
            "Processed 5 chunks from https://www.apdaparkinson.org/about-apda/david-g-standaert-md-phd\n",
            "Processed 6 chunks from https://www.apdaparkinson.org/about-apda/rebecca-gilbert-md-phd\n",
            "Processed 12 chunks from https://www.apdaparkinson.org/resources-support/living-with-parkinsons-disease/exercise\n",
            "Processed 12 chunks from https://www.apdaparkinson.org/resources-support/living-with-parkinsons-disease/diet-nutrition\n",
            "Processed 7 chunks from https://www.apdaparkinson.org/resources-support/living-with-parkinsons-disease/disability\n",
            "Processed 7 chunks from https://www.apdaparkinson.org/pdtheessentials\n",
            "Processed 37 chunks from https://www.apdaparkinson.org/resources-support/educational-video-library\n",
            "Processed 8 chunks from https://www.apdaparkinson.org/resources-support/local-resources\n",
            "Processed 6 chunks from https://www.apdaparkinson.org/article/fda-approves-new-adaptive-dbs-system\n",
            "Processed 10 chunks from https://www.apdaparkinson.org/article/american-parkinson-disease-association-free-two-day-virtual-conference-returns-to-educate-empower-and-engage-the-pd-communit\n",
            "Processed 9 chunks from https://www.apdaparkinson.org/article/american-parkinson-disease-association-to-educate-and-empower-during-parkinsons-disease-awareness-month\n",
            "Processed 9 chunks from https://www.apdaparkinson.org/article/american-parkinson-disease-association-kicks-off-2025-optimism-walk-series\n",
            "Processed 8 chunks from https://www.apdaparkinson.org/article/joining-forces-to-push-for-federal-research-funding\n",
            "Processed 8 chunks from https://www.apdaparkinson.org/apda-in-the-news\n",
            "Processed 9 chunks from https://www.apdaparkinson.org/article/golf-course-concerns-new-research-shows-link-to-parkinsons\n",
            "Processed 9 chunks from https://www.apdaparkinson.org/article/american-parkinson-disease-association-welcomes-two-accomplished-leaders-to-national-board-of-directors\n",
            "Processed 15 chunks from https://www.apdaparkinson.org/what-is-parkinsons/symptoms/sleep-problems\n",
            "Processed 12 chunks from https://www.apdaparkinson.org/what-is-parkinsons/symptoms/tremor\n",
            "Processed 8 chunks from https://www.apdaparkinson.org/what-is-parkinsons/symptoms/fatigue\n",
            "Processed 15 chunks from https://www.apdaparkinson.org/article/new-laboratory-tests-for-parkinsons-disease\n",
            "Processed 17 chunks from https://www.apdaparkinson.org/article/genetic-testing-for-parkinsons-disease\n",
            "Processed 7 chunks from https://www.apdaparkinson.org/videos/138935\n",
            "Processed 15 chunks from https://www.apdaparkinson.org/article/what-is-a-datscan-and-should-i-get-one\n",
            "Processed 7 chunks from https://www.apdaparkinson.org/what-is-parkinsons/symptoms/depression\n",
            "Processed 14 chunks from https://www.apdaparkinson.org/article/building-your-parkinsons-care-team\n",
            "Processed 13 chunks from https://www.apdaparkinson.org/article/the-gut-and-parkinsons\n",
            "Processed 14 chunks from https://www.apdaparkinson.org/article/the-relationship-between-pesticides-and-parkinsons\n",
            "   📄 Generated 955 text document chunks\n",
            "   🎬 Generated 237 media document chunks\n",
            "\n",
            "📂 Processing Parkinson Canada...\n",
            "Processed 7 chunks from https://www.parkinson.ca/\n",
            "Processed 8 chunks from https://www.parkinson.ca/parkinsons-iq-you\n",
            "Processed 7 chunks from https://www.parkinson.ca/about-us/leadership-team\n",
            "Processed 7 chunks from https://www.parkinson.ca/fr\n",
            "Processed 8 chunks from https://www.parkinson.ca/home/living-better/newly-diagnosed/how-to-tell-people\n",
            "Processed 16 chunks from https://www.parkinson.ca/home/living-better/newly-diagnosed/managing-stigma\n",
            "Processed 11 chunks from https://www.parkinson.ca/research/research-goals\n",
            "Processed 9 chunks from https://www.parkinson.ca/how-mime-over-mind-is-transforming-lives-through-creativity\n",
            "Processed 8 chunks from https://www.parkinson.ca/get-involved\n",
            "Processed 7 chunks from https://www.parkinson.ca/about-us/accountability/complaints-policy-and-procedure\n",
            "Processed 10 chunks from https://www.parkinson.ca/get-involved/careers\n",
            "Processed 11 chunks from https://www.parkinson.ca/home/living-better/staying-active\n",
            "Processed 10 chunks from https://www.parkinson.ca/what-is-parkinsons/progression\n",
            "Processed 6 chunks from https://www.parkinson.ca/care-finder\n",
            "Processed 32 chunks from https://www.parkinson.ca/what-is-parkinsons/atypical-parkinsonisms\n",
            "Processed 7 chunks from https://www.parkinson.ca/home/living-better\n",
            "Processed 7 chunks from https://www.parkinson.ca/about-us/contact-us\n",
            "Processed 20 chunks from https://www.parkinson.ca/research/apply-for-funding\n",
            "Processed 7 chunks from https://www.parkinson.ca/peer-support-program\n",
            "Processed 13 chunks from https://www.parkinson.ca/gait-challenges-in-parkinsons-new-research-directions\n",
            "Processed 8 chunks from https://www.parkinson.ca/about-us\n",
            "Processed 9 chunks from https://www.parkinson.ca/what-is-parkinsons/early-onset\n",
            "Processed 8 chunks from https://www.parkinson.ca/resources/support-groups\n",
            "Processed 11 chunks from https://www.parkinson.ca/what-is-parkinsons/motor-symptoms\n",
            "Processed 7 chunks from https://www.parkinson.ca\n",
            "Processed 9 chunks from https://www.parkinson.ca/get-involved/personal-giving\n",
            "Processed 27 chunks from https://www.parkinson.ca/privacy-policy\n",
            "Processed 7 chunks from https://www.parkinson.ca/get-involved/moment-maker\n",
            "Processed 8 chunks from https://www.parkinson.ca/about-us/accountability\n",
            "Processed 21 chunks from https://www.parkinson.ca/terms-and-conditions\n",
            "Processed 8 chunks from https://www.parkinson.ca/home/living-better/newly-diagnosed\n",
            "Processed 6 chunks from https://www.parkinson.ca/research/funded-research\n",
            "Processed 7 chunks from https://www.parkinson.ca/home/living-better/care-partners\n",
            "Processed 7 chunks from https://www.parkinson.ca/get-involved/volunteer\n",
            "Processed 9 chunks from https://www.parkinson.ca/care-fund\n",
            "Processed 14 chunks from https://www.parkinson.ca/home/living-better/build-your-team\n",
            "Processed 9 chunks from https://www.parkinson.ca/what-is-parkinsons\n",
            "Processed 9 chunks from https://www.parkinson.ca/research/canadian-open-parkinson-network\n",
            "Processed 8 chunks from https://www.parkinson.ca/research\n",
            "Processed 7 chunks from https://www.parkinson.ca/resources/blog\n",
            "Processed 10 chunks from https://www.parkinson.ca/dr-google-and-how-to-spot-parkinsons-misinformation\n",
            "Processed 13 chunks from https://www.parkinson.ca/what-is-parkinsons/non-motor-symptoms\n",
            "Processed 7 chunks from https://www.parkinson.ca/research/for-researchers\n",
            "Processed 7 chunks from https://www.parkinson.ca/about-us/media\n",
            "Processed 7 chunks from https://www.parkinson.ca/about-us/strategic-plan\n",
            "Processed 7 chunks from https://www.parkinson.ca/about-us/mission-vision-values\n",
            "Processed 7 chunks from https://www.parkinson.ca/resources\n",
            "Processed 7 chunks from https://www.parkinson.ca/about-us/financial-and-impact-reports\n",
            "Processed 7 chunks from https://www.parkinson.ca/resources/support-line\n",
            "Processed 7 chunks from https://www.parkinson.ca/research/new-in-research\n",
            "Processed 7 chunks from https://www.parkinson.ca/resources/community-events-and-webinars\n",
            "Processed 13 chunks from https://www.parkinson.ca/what-is-parkinsons/treatments\n",
            "Processed 9 chunks from https://www.parkinson.ca/resources/educational-publications/every-victory-counts-canadian-edition\n",
            "Processed 12 chunks from https://www.parkinson.ca/home/living-better/women\n",
            "Processed 8 chunks from https://www.parkinson.ca/resources/educational-publications\n",
            "Processed 9 chunks from https://www.parkinson.ca/get-involved/fundraising-events\n",
            "Processed 8 chunks from https://www.parkinson.ca/fr/parkinsons-iq-you\n",
            "Processed 8 chunks from https://www.parkinson.ca/fr/a-propos-de-nous/equipe-de-direction\n",
            "Processed 7 chunks from https://www.parkinson.ca/about-us/board-of-directors\n",
            "Processed 7 chunks from https://www.parkinson.ca/about-us/research-and-clinical-advisory-committee\n",
            "Processed 8 chunks from https://www.parkinson.ca/about-us/advisory-council\n",
            "Processed 10 chunks from https://www.parkinson.ca/fr/facons-de-donner/don-personnel\n",
            "Processed 11 chunks from https://www.parkinson.ca/fr/prasinezumab-un-medicament-prometteur-contre-la-maladie-de-parkinson-entre-dans-la-phase-suivante-des-essais-cliniques\n",
            "Processed 8 chunks from https://www.parkinson.ca/fr/ressources/ligne-de-soutien-pour-la-maladie-de-parkinson\n",
            "Processed 10 chunks from https://www.parkinson.ca/fr/quest-ce-que-la-maladie-de-parkinson/debut-de-la-maladie\n",
            "Processed 10 chunks from https://www.parkinson.ca/fr/facons-de-donner/evenements-de-collecte-de-fonds\n",
            "Processed 9 chunks from https://www.parkinson.ca/fr/recherche/reseau-canadien-ouvert-de-la-maladie-de-parkinson\n",
            "Processed 10 chunks from https://www.parkinson.ca/fr/quest-ce-que-la-maladie-de-parkinson\n",
            "Processed 13 chunks from https://www.parkinson.ca/fr/facons-de-donner/opportunites-de-carriere\n",
            "Processed 13 chunks from https://www.parkinson.ca/fr/home/mieux-vivre-avec-la-maladie-de-parkinson/rester-actif\n",
            "Processed 7 chunks from https://www.parkinson.ca/fr/a-propos-de-nous/mission-vision-valeurs\n",
            "Processed 7 chunks from https://www.parkinson.ca/fr/a-propos-de-nous/rapports-financiers-et-dimpact\n",
            "Processed 11 chunks from https://www.parkinson.ca/fr/recherche/objectifs-de-la-recherche\n",
            "Processed 9 chunks from https://www.parkinson.ca/fr/recherche/pour-les-chercheurs\n",
            "Processed 10 chunks from https://www.parkinson.ca/fr/home/mieux-vivre-avec-la-maladie-de-parkinson/diagnostic-recent-de-la-maladie-de-parkinson\n",
            "Processed 15 chunks from https://www.parkinson.ca/fr/home/mieux-vivre-avec-la-maladie-de-parkinson/les-femmes-et-la-maladie-de-parkinson\n",
            "Processed 16 chunks from https://www.parkinson.ca/fr/quest-ce-que-la-maladie-de-parkinson/traitements-de-la-maladie-de-parkinson\n",
            "Processed 9 chunks from https://www.parkinson.ca/fr/ressources/publications-educatives\n",
            "Processed 17 chunks from https://www.parkinson.ca/fr/quest-ce-que-la-maladie-de-parkinson/symptomes-non-moteurs\n",
            "Processed 8 chunks from https://www.parkinson.ca/fr/ressources/evenements-communautaires-et-webinaires\n",
            "Processed 9 chunks from https://www.parkinson.ca/fr/a-propos-de-nous/salle-des-medias\n",
            "Processed 8 chunks from https://www.parkinson.ca/fr/programme-de-soutien-par-les-pairs\n",
            "Processed 8 chunks from https://www.parkinson.ca/fr/ressources/groupes-de-soutien\n",
            "Processed 7 chunks from https://www.parkinson.ca/fr/defense-des-interests\n",
            "Processed 8 chunks from https://www.parkinson.ca/fr/facons-de-donner/createur-de-moments\n",
            "Processed 6 chunks from https://www.parkinson.ca/fr/clinial-trial\n",
            "Processed 8 chunks from https://www.parkinson.ca/fr/ressources\n",
            "Processed 14 chunks from https://www.parkinson.ca/fr/home/mieux-vivre-avec-la-maladie-de-parkinson/constituez-votre-equipe\n",
            "Processed 7 chunks from https://www.parkinson.ca/fr/home/mieux-vivre-avec-la-maladie-de-parkinson/fournir-des-soins\n",
            "Processed 7 chunks from https://www.parkinson.ca/fr/facons-de-donner\n",
            "Processed 38 chunks from https://www.parkinson.ca/fr/quest-ce-que-la-maladie-de-parkinson/parkinsonismes-atypiques\n",
            "Processed 7 chunks from https://www.parkinson.ca/fr/recherche/recherche-financee\n",
            "Processed 7 chunks from https://www.parkinson.ca/fr/a-propos-de-nous/plan-strategique\n",
            "Processed 8 chunks from https://www.parkinson.ca/fr/a-propos-de-nous/responsabilite\n",
            "Processed 8 chunks from https://www.parkinson.ca/fr/home/mieux-vivre-avec-la-maladie-de-parkinson\n",
            "Processed 14 chunks from https://www.parkinson.ca/fr/defis-de-la-marche-dans-la-maladie-de-parkinson-nouvelles-orientations-de-la-recherche\n",
            "Processed 19 chunks from https://www.parkinson.ca/fr/home/mieux-vivre-avec-la-maladie-de-parkinson/diagnostic-recent-de-la-maladie-de-parkinson/gerer-la-stigmatisation\n",
            "Processed 8 chunks from https://www.parkinson.ca/fr/a-propos-de-nous\n",
            "Processed 11 chunks from https://www.parkinson.ca/fr/ressources/publications-educatives/chaque-victoire-compte-edition-canadienne\n",
            "Processed 9 chunks from https://www.parkinson.ca/fr/comment-mime-over-mind-transforme-des-vies-grace-a-la-creativite\n",
            "   📄 Generated 999 text document chunks\n",
            "   🎬 Generated 29 media document chunks\n",
            "\n",
            "📂 Processing European Parkinson's Disease Association...\n",
            "   📄 Generated 0 text document chunks\n",
            "   🎬 Generated 0 media document chunks\n",
            "\n",
            "📂 Processing Parkinson's UK...\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/\n",
            "Processed 13 chunks from https://www.parkinsons.org.uk/about-us/benefits-working-us\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/research/parkinsons-virtual-biotech\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/information-and-support/local-groups\n",
            "Processed 12 chunks from https://www.parkinsons.org.uk/professionals/resources/helping-healthcare-professionals-share-research-patients\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/get-involved/fundraising\n",
            "Processed 9 chunks from https://www.parkinsons.org.uk/information-and-support/therapies-parkinsons\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/get-involved/order-fundraising-guide\n",
            "Processed 16 chunks from https://www.parkinsons.org.uk/professionals/resources/time-critical-medication-and-get-it-time-campaign-resources\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/about-us\n",
            "Processed 12 chunks from https://www.parkinsons.org.uk/get-involved/corporate-fundraising\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/about-us/feedback-and-complaints\n",
            "Processed 12 chunks from https://www.parkinsons.org.uk/information-and-support/young-onset-parkinsons\n",
            "Processed 9 chunks from https://www.parkinsons.org.uk/about-us/media-and-press-office\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/professionals/parkinsons-learning-pathway-pharmacy-professionals\n",
            "Processed 17 chunks from https://www.parkinsons.org.uk/about-us/terms-and-conditions\n",
            "Processed 12 chunks from https://www.parkinsons.org.uk/information-and-support/parkinsons-nurses\n",
            "Processed 40 chunks from https://www.parkinsons.org.uk/about-us/privacy-policy\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/get-involved/volunteer-parkinsons-uk\n",
            "Processed 20 chunks from https://www.parkinsons.org.uk/information-and-support/helpline-and-parkinsons-advisers\n",
            "Processed 8 chunks from https://www.parkinsons.org.uk/information-and-support/your-magazine\n",
            "Processed 11 chunks from https://www.parkinsons.org.uk/get-involved/support-research-breakthroughs\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/about-us/parkinsons-uk-northern-ireland\n",
            "Processed 11 chunks from https://www.parkinsons.org.uk/information-and-support/side-effects-parkinsons-drugs\n",
            "Processed 13 chunks from https://www.parkinsons.org.uk/research/resources-and-support-researchers\n",
            "Processed 11 chunks from https://www.parkinsons.org.uk/get-involved/give-memory\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/about-us/what-we-do\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/about-us/supporter-enquiries\n",
            "Processed 12 chunks from https://www.parkinsons.org.uk/research/when-will-there-be-cure-parkinsons\n",
            "Processed 12 chunks from https://www.parkinsons.org.uk/information-and-support/newly-diagnosed-parkinsons\n",
            "Processed 9 chunks from https://www.parkinsons.org.uk/professionals/parkinsons-excellence-network-awards\n",
            "Processed 11 chunks from https://www.parkinsons.org.uk/information-and-support/supporting-someone-parkinsons\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/professionals/resources\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk\n",
            "Processed 8 chunks from https://www.parkinsons.org.uk/professionals/local-parkinsons-excellence-networks-and-excellence-hubs\n",
            "Processed 9 chunks from https://www.parkinsons.org.uk/information-and-support/relationships-and-family-life\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/get-involved/do-your-own-fundraising\n",
            "Processed 12 chunks from https://www.parkinsons.org.uk/about-us/what-we-think\n",
            "Processed 11 chunks from https://www.parkinsons.org.uk/research/research-volunteering-opportunities\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/research/parkinsons-researchers\n",
            "Processed 11 chunks from https://www.parkinsons.org.uk/research/our-research-projects\n",
            "Processed 9 chunks from https://www.parkinsons.org.uk/about-us/general-enquiries\n",
            "Processed 11 chunks from https://www.parkinsons.org.uk/professionals/learning-hub\n",
            "Processed 12 chunks from https://www.parkinsons.org.uk/information-and-support/does-parkinsons-run-families\n",
            "Processed 12 chunks from https://www.parkinsons.org.uk/information-and-support/deep-brain-stimulation\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/get-involved/membership\n",
            "Processed 20 chunks from https://www.parkinsons.org.uk/professionals/uk-parkinsons-audit-transforming-care\n",
            "Processed 16 chunks from https://www.parkinsons.org.uk/information-and-support/parkinsons-drugs\n",
            "Processed 11 chunks from https://www.parkinsons.org.uk/information-and-support/parkinsons-symptoms\n",
            "Processed 18 chunks from https://www.parkinsons.org.uk/information-and-support/types-parkinsons\n",
            "Processed 11 chunks from https://www.parkinsons.org.uk/research/get-involved-research\n",
            "Processed 11 chunks from https://www.parkinsons.org.uk/information-and-support/work-money-driving-and-legal-rights-when-you-have-parkinsons\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/information-and-support/living-parkinsons\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/professionals/news\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/professionals/funding-opportunities\n",
            "Processed 9 chunks from https://www.parkinsons.org.uk/research/research-blog\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/news\n",
            "Processed 15 chunks from https://www.parkinsons.org.uk/get-involved/campaigning-change\n",
            "Processed 11 chunks from https://www.parkinsons.org.uk/about-us/join-team-parkinsons\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/get-involved/where-your-money-goes\n",
            "Processed 8 chunks from https://www.parkinsons.org.uk/professionals/most-popular-resources-your-patients\n",
            "Processed 15 chunks from https://www.parkinsons.org.uk/contact\n",
            "Processed 11 chunks from https://www.parkinsons.org.uk/get-involved/agm\n",
            "Processed 4 chunks from https://www.parkinsons.org.uk/get-involved/volunteering-roles\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/news/news-topics/research\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/get-involved/our-health-and-social-care-campaigns\n",
            "Processed 8 chunks from https://www.parkinsons.org.uk/information-and-support/treatments-and-therapies-parkinsons\n",
            "Processed 9 chunks from https://www.parkinsons.org.uk/get-involved/volunteer-induction\n",
            "Processed 1 chunks from https://www.parkinsons.org.uk/cdn-cgi/l/email-protection\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/get-involved/how-send-us-your-fundraising\n",
            "Processed 11 chunks from https://www.parkinsons.org.uk/professionals/information-your-patients\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/professionals/parkinsons-learning-pathway-health-care-staff\n",
            "Processed 14 chunks from https://www.parkinsons.org.uk/information-and-support/parkinsons-and-mental-health\n",
            "Processed 9 chunks from https://www.parkinsons.org.uk/information-and-support/our-support-services\n",
            "Processed 15 chunks from https://www.parkinsons.org.uk/about-us/our-strategy\n",
            "Processed 8 chunks from https://www.parkinsons.org.uk/get-involved/assemble-user-guide\n",
            "Processed 9 chunks from https://www.parkinsons.org.uk/information-and-support/motor-symptoms-parkinsons\n",
            "Processed 13 chunks from https://www.parkinsons.org.uk/about-us/jobs-parkinsons-uk\n",
            "Processed 12 chunks from https://www.parkinsons.org.uk/get-involved/volunteer-resources\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/information-and-support/non-motor-symptoms-parkinsons\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/professionals/parkinsons-therapist-learning-pathway\n",
            "Processed 13 chunks from https://www.parkinsons.org.uk/professionals/parkinsons-uk-excellence-network\n",
            "Processed 11 chunks from https://www.parkinsons.org.uk/information-and-support/what-parkinsons\n",
            "Processed 8 chunks from https://www.parkinsons.org.uk/professionals/clinical-tools-and-assessments\n",
            "Processed 17 chunks from https://www.parkinsons.org.uk/research/research-events\n",
            "Processed 12 chunks from https://www.parkinsons.org.uk/information-and-support/how-does-parkinsons-progress\n",
            "Processed 17 chunks from https://www.parkinsons.org.uk/research/patient-and-public-involvement-research\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/professionals/events-and-learning\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/get-involved/sign-parky-charter\n",
            "Processed 9 chunks from https://www.parkinsons.org.uk/information-and-support/everyday-life\n",
            "Processed 12 chunks from https://www.parkinsons.org.uk/research/explore-our-research\n",
            "Processed 11 chunks from https://www.parkinsons.org.uk/information-and-support/what-causes-parkinsons\n",
            "Processed 11 chunks from https://www.parkinsons.org.uk/get-involved/make-benefits-work-people-parkinsons\n",
            "Processed 10 chunks from https://www.parkinsons.org.uk/professionals/resources-professionals\n",
            "Processed 7 chunks from https://www.parkinsons.org.uk/events\n",
            "Processed 11 chunks from https://www.parkinsons.org.uk/get-involved/become-member\n",
            "Processed 13 chunks from https://www.parkinsons.org.uk/about-us/cookies-policy\n",
            "Processed 12 chunks from https://www.parkinsons.org.uk/get-involved/parkinsons-uk-parliament\n",
            "Processed 12 chunks from https://www.parkinsons.org.uk/research/staying-connected-your-participants\n",
            "Processed 13 chunks from https://www.parkinsons.org.uk/research/our-research-impact\n",
            "   📄 Generated 1133 text document chunks\n",
            "   🎬 Generated 64 media document chunks\n",
            "\n",
            "📂 Processing Davis Phinney Foundation...\n",
            "   📄 Generated 0 text document chunks\n",
            "   🎬 Generated 0 media document chunks\n",
            "\n",
            "📂 Processing PMD Alliance...\n",
            "Processed 8 chunks from https://www.pmdalliance.org/\n",
            "Processed 10 chunks from https://www.pmdalliance.org/board-members\n",
            "Processed 7 chunks from https://www.pmdalliance.org/medication-access\n",
            "Processed 6 chunks from https://www.pmdalliance.org/impactful-information\n",
            "Processed 5 chunks from https://www.pmdalliance.org/contact-us\n",
            "Processed 8 chunks from https://www.pmdalliance.org/make-an-impact/ways-to-build-a-legacy\n",
            "Processed 8 chunks from https://www.pmdalliance.org/recently-diagnosed\n",
            "Processed 8 chunks from https://www.pmdalliance.org/news\n",
            "Processed 6 chunks from https://www.pmdalliance.org/careers\n",
            "Processed 7 chunks from https://www.pmdalliance.org/ecosystem\n",
            "Processed 8 chunks from https://www.pmdalliance.org/all-in-summit\n",
            "Processed 6 chunks from https://www.pmdalliance.org/resources\n",
            "Processed 6 chunks from https://www.pmdalliance.org/clinical-partners\n",
            "Processed 6 chunks from https://www.pmdalliance.org/community-partners\n",
            "Processed 19 chunks from https://www.pmdalliance.org/staff\n",
            "Processed 8 chunks from https://www.pmdalliance.org/my-pd-care\n",
            "Processed 7 chunks from https://www.pmdalliance.org/featured-support-groups\n",
            "Processed 14 chunks from https://www.pmdalliance.org/privacy-policy\n",
            "Processed 8 chunks from https://www.pmdalliance.org/events\n",
            "Processed 7 chunks from https://www.pmdalliance.org/treatmentoptions/themoreyouknow\n",
            "Processed 8 chunks from https://www.pmdalliance.org/ways-to-donate\n",
            "Processed 7 chunks from https://www.pmdalliance.org/yopd\n",
            "Processed 5 chunks from https://www.pmdalliance.org/about\n",
            "Processed 17 chunks from https://www.pmdalliance.org/treatmentoptions/medication\n",
            "Processed 9 chunks from https://www.pmdalliance.org/margaret-tuchman-library\n",
            "Processed 12 chunks from https://www.pmdalliance.org/care-partners-family-members-adult-children\n",
            "Processed 16 chunks from https://www.pmdalliance.org/sabes\n",
            "Processed 6 chunks from https://www.pmdalliance.org/make-an-impact/impactful-information\n",
            "Processed 6 chunks from https://www.pmdalliance.org/realtalk\n",
            "Processed 7 chunks from https://www.pmdalliance.org/la-comision-asesora-para-parkinson\n",
            "Processed 6 chunks from https://www.pmdalliance.org/partnerships\n",
            "Processed 12 chunks from https://www.pmdalliance.org/person-with-movement-disorder\n",
            "Processed 5 chunks from https://www.pmdalliance.org/resources/national-regional-local\n",
            "Processed 6 chunks from https://www.pmdalliance.org/treatmentoptions\n",
            "Processed 6 chunks from https://www.pmdalliance.org/empowered-tool\n",
            "Processed 5 chunks from https://www.pmdalliance.org/about-us\n",
            "Processed 6 chunks from https://www.pmdalliance.org/healthcare-providers\n",
            "Processed 6 chunks from https://www.pmdalliance.org/industry-partners\n",
            "Processed 8 chunks from https://www.pmdalliance.org/missionvisionandvalues\n",
            "Processed 10 chunks from https://www.pmdalliance.org/support-group-leader\n",
            "Processed 10 chunks from https://www.pmdalliance.org/cpdc\n",
            "Processed 8 chunks from https://www.pmdalliance.org/treatmentoptions/device-aided-therapies\n",
            "Processed 5 chunks from https://www.pmdalliance.org/give\n",
            "Processed 7 chunks from https://www.pmdalliance.org/press-room\n",
            "Processed 9 chunks from https://www.pmdalliance.org/neuro-life-online-video-library\n",
            "Processed 10 chunks from https://www.pmdalliance.org/certified-parkinson-disease-care-facilities\n",
            "Processed 8 chunks from https://www.pmdalliance.org\n",
            "Processed 12 chunks from https://www.pmdalliance.org/ambassadors\n",
            "Processed 8 chunks from https://www.pmdalliance.org/campfire\n",
            "Processed 10 chunks from https://www.pmdalliance.org/about/board-members\n",
            "Processed 7 chunks from https://www.pmdalliance.org/about/ecosystem\n",
            "Processed 12 chunks from https://www.pmdalliance.org/about/ambassadors\n",
            "Processed 5 chunks from https://www.pmdalliance.org/make-an-impact/impactful-information/form-990\n",
            "Processed 6 chunks from https://www.pmdalliance.org/make-an-impact/impactful-information/quarterly-impact-reports\n",
            "Processed 6 chunks from https://www.pmdalliance.org/impactful-information/annualreports\n",
            "Processed 8 chunks from https://www.pmdalliance.org/make-an-impact/impactful-information/donor-bill-of-rights\n",
            "Processed 12 chunks from https://www.pmdalliance.org/2025/04/30/yoga-manual-for-pd\n",
            "Processed 15 chunks from https://www.pmdalliance.org/2025/06/10/tiktok-yopd-community\n",
            "Processed 7 chunks from https://www.pmdalliance.org/news/page/3\n",
            "Processed 15 chunks from https://www.pmdalliance.org/2025/04/30/space-for-pd-care-partners\n",
            "Processed 9 chunks from https://www.pmdalliance.org/2025/06/18/10-ice-cold-summer-beverages\n",
            "Processed 7 chunks from https://www.pmdalliance.org/news/page/32\n",
            "Processed 11 chunks from https://www.pmdalliance.org/2025/06/04/10-parkinsons-books\n",
            "Processed 8 chunks from https://www.pmdalliance.org/news/page/2\n",
            "Processed 8 chunks from https://www.pmdalliance.org/2025/05/28/recognition-roundup-june-cpdc\n",
            "Processed 9 chunks from https://www.pmdalliance.org/2025/05/01/learner-rev-urinary-challenges\n",
            "Processed 9 chunks from https://www.pmdalliance.org/2025/06/03/weary-warrior\n",
            "Processed 15 chunks from https://www.pmdalliance.org/2025/06/03/movement-disorder-care-mt-nv-wy\n",
            "Processed 6 chunks from https://www.pmdalliance.org/about/vision/ecosystem-the-companion-network\n",
            "Processed 7 chunks from https://www.pmdalliance.org/about/vision/ecosystem-the-community-network\n",
            "Processed 5 chunks from https://www.pmdalliance.org/portfolio/founders-story\n",
            "Processed 6 chunks from https://www.pmdalliance.org/about/vision/ecosystem-the-family-network\n",
            "Processed 10 chunks from https://www.pmdalliance.org/about/vision/ecosystem-the-medical-network\n",
            "Processed 5 chunks from https://www.pmdalliance.org/portfolio/an-interview-with-pmd-alliance-founders-sarah-jones-and-judy-talley\n",
            "Processed 9 chunks from https://www.pmdalliance.org/about/vision/ecosystem-the-therapy-network\n",
            "Processed 10 chunks from https://www.pmdalliance.org/2024/12/04/all-in-24-recap\n",
            "Processed 39 chunks from https://www.pmdalliance.org/all-in-summit/speakers\n",
            "Processed 5 chunks from https://www.pmdalliance.org/resources/resources-national-regional-local-resources\n",
            "Processed 96 chunks from https://www.pmdalliance.org/bldg-your-med-network\n",
            "Processed 8 chunks from http://www.pmdalliance.org\n",
            "Processed 6 chunks from https://www.pmdalliance.org/carefree-retreat\n",
            "Processed 5 chunks from https://www.pmdalliance.org/portfolio/the-more-you-know-amantadine\n",
            "Processed 5 chunks from https://www.pmdalliance.org/portfolio/the-more-you-know-xeomin\n",
            "Processed 5 chunks from https://www.pmdalliance.org/portfolio/the-more-you-know-nuplazid\n",
            "Processed 5 chunks from https://www.pmdalliance.org/portfolio/the-more-you-know-exelon\n",
            "Processed 5 chunks from https://www.pmdalliance.org/portfolio/the-more-you-know-ongentys\n",
            "Processed 5 chunks from https://www.pmdalliance.org/portfolio/the-more-you-know-cogentin\n",
            "Processed 5 chunks from https://www.pmdalliance.org/portfolio/the-more-you-know-apokyn\n",
            "Processed 5 chunks from https://www.pmdalliance.org/portfolio/the-more-you-know-nourianz\n",
            "Processed 5 chunks from https://www.pmdalliance.org/portfolio/the-more-you-know-neupro\n",
            "Processed 5 chunks from https://www.pmdalliance.org/portfolio/the-more-you-know-gocovri\n",
            "Processed 5 chunks from https://www.pmdalliance.org/portfolio/the-more-you-know-inbrija\n",
            "Processed 5 chunks from https://www.pmdalliance.org/portfolio/the-more-you-know-azilect\n",
            "Processed 5 chunks from https://www.pmdalliance.org/portfolio/the-more-you-know-northera\n",
            "Processed 6 chunks from https://www.pmdalliance.org/treatmentoptions/themoreyouknow/page/2\n",
            "Processed 5 chunks from https://www.pmdalliance.org/portfolio/the-more-you-know-mirapex\n",
            "Processed 5 chunks from https://www.pmdalliance.org/portfolio/the-more-you-know-rytary\n",
            "Processed 5 chunks from https://www.pmdalliance.org/portfolio/the-more-you-know-sinemet\n",
            "Processed 5 chunks from https://www.pmdalliance.org/portfolio/the-more-you-know-onapgo\n",
            "   📄 Generated 884 text document chunks\n",
            "   🎬 Generated 120 media document chunks\n",
            "\n",
            "📂 Processing ParkinsonNet...\n",
            "Processed 4 chunks from https://www.parkinsonnet.com/\n",
            "Processed 1 chunks from https://www.parkinsonnet.com/about-parkinsonnet/around-the-globe\n",
            "Processed 1 chunks from https://www.parkinsonnet.com/succes-stories/partner-of-person-with-pd\n",
            "Processed 1 chunks from https://www.parkinsonnet.com/discipline/speech-and-language\n",
            "Processed 3 chunks from https://www.parkinsonnet.com/discipline/physiotherapy\n",
            "Processed 2 chunks from https://www.parkinsonnet.com/about-parkinsonnet\n",
            "Processed 1 chunks from https://www.parkinsonnet.com/about-parkinsonnet/scientific-evidence\n",
            "Processed 1 chunks from https://www.parkinsonnet.com/discipline/nutrition\n",
            "Processed 4 chunks from https://www.parkinsonnet.com\n",
            "Processed 1 chunks from https://www.parkinsonnet.com/contact\n",
            "Processed 4 chunks from https://www.parkinsonnet.com/news/royal-honors-for-marten-munneke-and-bas-bloem\n",
            "Processed 3 chunks from https://www.parkinsonnet.com/news/letter-of-intent-signed-in-osnabruck\n",
            "Processed 1 chunks from https://www.parkinsonnet.com/succes-stories/christopher-c-distasio\n",
            "Processed 2 chunks from https://www.parkinsonnet.com/news/world-parkinson-congress-in-barcelona-a-recap\n",
            "Processed 3 chunks from https://www.parkinsonnet.com/about-parkinsonnet/our-team\n",
            "Processed 3 chunks from https://www.parkinsonnet.com/about-parkinsonnet/services\n",
            "Processed 3 chunks from https://www.parkinsonnet.com/news/lets-celebrate-parkinsonnet-20-years\n",
            "Processed 2 chunks from https://www.parkinsonnet.com/news/implementing-parkinsonnet-in-5th-gear\n",
            "Processed 1 chunks from https://www.parkinsonnet.com/discipline/for-people-with-parkinsons\n",
            "Processed 2 chunks from https://www.parkinsonnet.com/discipline/occupational-therapy\n",
            "Processed 3 chunks from https://www.parkinsonnet.com/guidelines\n",
            "Processed 2 chunks from https://www.parkinsonnet.com/news\n",
            "Processed 2 chunks from https://www.parkinsonnet.com/news/page/2\n",
            "Processed 4 chunks from https://www.parkinsonnet.com/news/parkinsonnet-care-providers-reduce-complications-in-parkinsons-disease\n",
            "Processed 1 chunks from https://www.parkinsonnet.com/news/page/3\n",
            "Processed 3 chunks from https://www.parkinsonnet.com/news/skills-web-2022-a-recap\n",
            "Processed 2 chunks from https://www.parkinsonnet.com/news/parkinsontv-the-long-road-to-hope\n",
            "Processed 2 chunks from https://www.parkinsonnet.com/news/staying-connected-online\n",
            "Processed 4 chunks from https://www.parkinsonnet.com/news/5-years-of-parkinsonnet-luxembourg\n",
            "   📄 Generated 66 text document chunks\n",
            "   🎬 Generated 10 media document chunks\n",
            "\n",
            "======================================================================\n",
            "📊 COMPLETE SUMMARY\n",
            "======================================================================\n",
            "📄 Text documents: 6655\n",
            "🎬 Media documents: 639\n",
            "📚 Total documents to store: 7294\n",
            "\n",
            "Storing ALL documents in ChromaDB with rate limiting and PERSISTENCE...\n",
            "Storing 7294 documents in batches of 20 with rate limiting\n",
            "Processing batch 1/365...\n",
            "✅ Stored batch 1/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 2/365...\n",
            "✅ Stored batch 2/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 3/365...\n",
            "✅ Stored batch 3/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 4/365...\n",
            "✅ Stored batch 4/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 5/365...\n",
            "✅ Stored batch 5/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 6/365...\n",
            "✅ Stored batch 6/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 7/365...\n",
            "✅ Stored batch 7/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 8/365...\n",
            "✅ Stored batch 8/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 9/365...\n",
            "✅ Stored batch 9/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 10/365...\n",
            "✅ Stored batch 10/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 11/365...\n",
            "✅ Stored batch 11/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 12/365...\n",
            "✅ Stored batch 12/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 13/365...\n",
            "✅ Stored batch 13/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 14/365...\n",
            "✅ Stored batch 14/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 15/365...\n",
            "✅ Stored batch 15/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 16/365...\n",
            "✅ Stored batch 16/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 17/365...\n",
            "✅ Stored batch 17/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 18/365...\n",
            "✅ Stored batch 18/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 19/365...\n",
            "✅ Stored batch 19/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 20/365...\n",
            "✅ Stored batch 20/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 21/365...\n",
            "✅ Stored batch 21/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 22/365...\n",
            "✅ Stored batch 22/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 23/365...\n",
            "✅ Stored batch 23/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 24/365...\n",
            "✅ Stored batch 24/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 25/365...\n",
            "✅ Stored batch 25/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 26/365...\n",
            "✅ Stored batch 26/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 27/365...\n",
            "✅ Stored batch 27/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 28/365...\n",
            "✅ Stored batch 28/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 29/365...\n",
            "✅ Stored batch 29/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 30/365...\n",
            "✅ Stored batch 30/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 31/365...\n",
            "✅ Stored batch 31/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 32/365...\n",
            "✅ Stored batch 32/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 33/365...\n",
            "✅ Stored batch 33/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 34/365...\n",
            "✅ Stored batch 34/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 35/365...\n",
            "✅ Stored batch 35/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 36/365...\n",
            "✅ Stored batch 36/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 37/365...\n",
            "✅ Stored batch 37/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 38/365...\n",
            "✅ Stored batch 38/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 39/365...\n",
            "✅ Stored batch 39/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 40/365...\n",
            "✅ Stored batch 40/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 41/365...\n",
            "✅ Stored batch 41/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 42/365...\n",
            "✅ Stored batch 42/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 43/365...\n",
            "✅ Stored batch 43/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 44/365...\n",
            "✅ Stored batch 44/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 45/365...\n",
            "✅ Stored batch 45/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 46/365...\n",
            "✅ Stored batch 46/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 47/365...\n",
            "✅ Stored batch 47/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 48/365...\n",
            "✅ Stored batch 48/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 49/365...\n",
            "✅ Stored batch 49/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 50/365...\n",
            "✅ Stored batch 50/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 51/365...\n",
            "✅ Stored batch 51/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 52/365...\n",
            "✅ Stored batch 52/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 53/365...\n",
            "✅ Stored batch 53/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 54/365...\n",
            "✅ Stored batch 54/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 55/365...\n",
            "✅ Stored batch 55/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 56/365...\n",
            "✅ Stored batch 56/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 57/365...\n",
            "✅ Stored batch 57/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 58/365...\n",
            "✅ Stored batch 58/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 59/365...\n",
            "✅ Stored batch 59/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 60/365...\n",
            "✅ Stored batch 60/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 61/365...\n",
            "✅ Stored batch 61/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 62/365...\n",
            "✅ Stored batch 62/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 63/365...\n",
            "✅ Stored batch 63/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 64/365...\n",
            "✅ Stored batch 64/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 65/365...\n",
            "✅ Stored batch 65/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 66/365...\n",
            "✅ Stored batch 66/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 67/365...\n",
            "✅ Stored batch 67/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 68/365...\n",
            "✅ Stored batch 68/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 69/365...\n",
            "✅ Stored batch 69/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 70/365...\n",
            "✅ Stored batch 70/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 71/365...\n",
            "✅ Stored batch 71/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 72/365...\n",
            "✅ Stored batch 72/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 73/365...\n",
            "✅ Stored batch 73/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 74/365...\n",
            "✅ Stored batch 74/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 75/365...\n",
            "✅ Stored batch 75/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 76/365...\n",
            "✅ Stored batch 76/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 77/365...\n",
            "✅ Stored batch 77/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 78/365...\n",
            "✅ Stored batch 78/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 79/365...\n",
            "✅ Stored batch 79/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 80/365...\n",
            "✅ Stored batch 80/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 81/365...\n",
            "✅ Stored batch 81/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 82/365...\n",
            "✅ Stored batch 82/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 83/365...\n",
            "✅ Stored batch 83/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 84/365...\n",
            "✅ Stored batch 84/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 85/365...\n",
            "✅ Stored batch 85/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 86/365...\n",
            "✅ Stored batch 86/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 87/365...\n",
            "✅ Stored batch 87/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 88/365...\n",
            "✅ Stored batch 88/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 89/365...\n",
            "✅ Stored batch 89/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 90/365...\n",
            "✅ Stored batch 90/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 91/365...\n",
            "✅ Stored batch 91/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 92/365...\n",
            "✅ Stored batch 92/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 93/365...\n",
            "✅ Stored batch 93/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 94/365...\n",
            "✅ Stored batch 94/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 95/365...\n",
            "✅ Stored batch 95/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 96/365...\n",
            "✅ Stored batch 96/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 97/365...\n",
            "✅ Stored batch 97/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 98/365...\n",
            "✅ Stored batch 98/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 99/365...\n",
            "✅ Stored batch 99/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 100/365...\n",
            "✅ Stored batch 100/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 101/365...\n",
            "✅ Stored batch 101/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 102/365...\n",
            "✅ Stored batch 102/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 103/365...\n",
            "✅ Stored batch 103/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 104/365...\n",
            "✅ Stored batch 104/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 105/365...\n",
            "✅ Stored batch 105/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 106/365...\n",
            "✅ Stored batch 106/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 107/365...\n",
            "✅ Stored batch 107/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 108/365...\n",
            "✅ Stored batch 108/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 109/365...\n",
            "✅ Stored batch 109/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 110/365...\n",
            "✅ Stored batch 110/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 111/365...\n",
            "✅ Stored batch 111/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 112/365...\n",
            "✅ Stored batch 112/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 113/365...\n",
            "✅ Stored batch 113/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 114/365...\n",
            "✅ Stored batch 114/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 115/365...\n",
            "✅ Stored batch 115/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 116/365...\n",
            "✅ Stored batch 116/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 117/365...\n",
            "✅ Stored batch 117/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 118/365...\n",
            "✅ Stored batch 118/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 119/365...\n",
            "✅ Stored batch 119/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 120/365...\n",
            "✅ Stored batch 120/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 121/365...\n",
            "✅ Stored batch 121/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 122/365...\n",
            "✅ Stored batch 122/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 123/365...\n",
            "✅ Stored batch 123/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 124/365...\n",
            "✅ Stored batch 124/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 125/365...\n",
            "✅ Stored batch 125/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 126/365...\n",
            "✅ Stored batch 126/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 127/365...\n",
            "✅ Stored batch 127/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 128/365...\n",
            "✅ Stored batch 128/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 129/365...\n",
            "✅ Stored batch 129/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 130/365...\n",
            "✅ Stored batch 130/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 131/365...\n",
            "✅ Stored batch 131/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 132/365...\n",
            "✅ Stored batch 132/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 133/365...\n",
            "✅ Stored batch 133/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 134/365...\n",
            "✅ Stored batch 134/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 135/365...\n",
            "✅ Stored batch 135/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 136/365...\n",
            "✅ Stored batch 136/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 137/365...\n",
            "✅ Stored batch 137/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 138/365...\n",
            "✅ Stored batch 138/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 139/365...\n",
            "✅ Stored batch 139/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 140/365...\n",
            "✅ Stored batch 140/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 141/365...\n",
            "✅ Stored batch 141/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 142/365...\n",
            "✅ Stored batch 142/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 143/365...\n",
            "✅ Stored batch 143/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 144/365...\n",
            "✅ Stored batch 144/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 145/365...\n",
            "✅ Stored batch 145/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 146/365...\n",
            "✅ Stored batch 146/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 147/365...\n",
            "✅ Stored batch 147/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 148/365...\n",
            "✅ Stored batch 148/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 149/365...\n",
            "✅ Stored batch 149/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 150/365...\n",
            "✅ Stored batch 150/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 151/365...\n",
            "✅ Stored batch 151/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 152/365...\n",
            "✅ Stored batch 152/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 153/365...\n",
            "✅ Stored batch 153/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 154/365...\n",
            "✅ Stored batch 154/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 155/365...\n",
            "✅ Stored batch 155/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 156/365...\n",
            "✅ Stored batch 156/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 157/365...\n",
            "✅ Stored batch 157/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 158/365...\n",
            "✅ Stored batch 158/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 159/365...\n",
            "✅ Stored batch 159/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 160/365...\n",
            "✅ Stored batch 160/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 161/365...\n",
            "✅ Stored batch 161/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 162/365...\n",
            "✅ Stored batch 162/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 163/365...\n",
            "✅ Stored batch 163/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 164/365...\n",
            "✅ Stored batch 164/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 165/365...\n",
            "✅ Stored batch 165/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 166/365...\n",
            "✅ Stored batch 166/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 167/365...\n",
            "✅ Stored batch 167/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 168/365...\n",
            "✅ Stored batch 168/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 169/365...\n",
            "✅ Stored batch 169/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 170/365...\n",
            "✅ Stored batch 170/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 171/365...\n",
            "✅ Stored batch 171/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 172/365...\n",
            "✅ Stored batch 172/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 173/365...\n",
            "✅ Stored batch 173/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 174/365...\n",
            "✅ Stored batch 174/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 175/365...\n",
            "✅ Stored batch 175/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 176/365...\n",
            "✅ Stored batch 176/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 177/365...\n",
            "✅ Stored batch 177/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 178/365...\n",
            "✅ Stored batch 178/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 179/365...\n",
            "✅ Stored batch 179/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 180/365...\n",
            "✅ Stored batch 180/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 181/365...\n",
            "✅ Stored batch 181/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 182/365...\n",
            "✅ Stored batch 182/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 183/365...\n",
            "✅ Stored batch 183/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 184/365...\n",
            "✅ Stored batch 184/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 185/365...\n",
            "✅ Stored batch 185/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 186/365...\n",
            "✅ Stored batch 186/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 187/365...\n",
            "✅ Stored batch 187/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 188/365...\n",
            "✅ Stored batch 188/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 189/365...\n",
            "✅ Stored batch 189/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 190/365...\n",
            "✅ Stored batch 190/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 191/365...\n",
            "✅ Stored batch 191/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 192/365...\n",
            "✅ Stored batch 192/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 193/365...\n",
            "✅ Stored batch 193/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 194/365...\n",
            "✅ Stored batch 194/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 195/365...\n",
            "✅ Stored batch 195/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 196/365...\n",
            "✅ Stored batch 196/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 197/365...\n",
            "✅ Stored batch 197/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 198/365...\n",
            "✅ Stored batch 198/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 199/365...\n",
            "✅ Stored batch 199/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 200/365...\n",
            "✅ Stored batch 200/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 201/365...\n",
            "✅ Stored batch 201/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 202/365...\n",
            "✅ Stored batch 202/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 203/365...\n",
            "✅ Stored batch 203/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 204/365...\n",
            "✅ Stored batch 204/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 205/365...\n",
            "✅ Stored batch 205/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 206/365...\n",
            "✅ Stored batch 206/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 207/365...\n",
            "✅ Stored batch 207/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 208/365...\n",
            "✅ Stored batch 208/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 209/365...\n",
            "✅ Stored batch 209/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 210/365...\n",
            "✅ Stored batch 210/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 211/365...\n",
            "✅ Stored batch 211/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 212/365...\n",
            "✅ Stored batch 212/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 213/365...\n",
            "✅ Stored batch 213/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 214/365...\n",
            "✅ Stored batch 214/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 215/365...\n",
            "✅ Stored batch 215/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 216/365...\n",
            "✅ Stored batch 216/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 217/365...\n",
            "✅ Stored batch 217/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 218/365...\n",
            "✅ Stored batch 218/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 219/365...\n",
            "✅ Stored batch 219/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 220/365...\n",
            "✅ Stored batch 220/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 221/365...\n",
            "✅ Stored batch 221/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 222/365...\n",
            "✅ Stored batch 222/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 223/365...\n",
            "✅ Stored batch 223/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 224/365...\n",
            "✅ Stored batch 224/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 225/365...\n",
            "✅ Stored batch 225/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 226/365...\n",
            "✅ Stored batch 226/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 227/365...\n",
            "✅ Stored batch 227/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 228/365...\n",
            "✅ Stored batch 228/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 229/365...\n",
            "✅ Stored batch 229/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 230/365...\n",
            "✅ Stored batch 230/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 231/365...\n",
            "✅ Stored batch 231/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 232/365...\n",
            "✅ Stored batch 232/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 233/365...\n",
            "✅ Stored batch 233/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 234/365...\n",
            "✅ Stored batch 234/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 235/365...\n",
            "✅ Stored batch 235/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 236/365...\n",
            "✅ Stored batch 236/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 237/365...\n",
            "✅ Stored batch 237/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 238/365...\n",
            "✅ Stored batch 238/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 239/365...\n",
            "✅ Stored batch 239/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 240/365...\n",
            "✅ Stored batch 240/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 241/365...\n",
            "✅ Stored batch 241/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 242/365...\n",
            "✅ Stored batch 242/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 243/365...\n",
            "✅ Stored batch 243/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 244/365...\n",
            "✅ Stored batch 244/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 245/365...\n",
            "✅ Stored batch 245/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 246/365...\n",
            "✅ Stored batch 246/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 247/365...\n",
            "✅ Stored batch 247/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 248/365...\n",
            "✅ Stored batch 248/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 249/365...\n",
            "✅ Stored batch 249/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 250/365...\n",
            "✅ Stored batch 250/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 251/365...\n",
            "✅ Stored batch 251/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 252/365...\n",
            "✅ Stored batch 252/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 253/365...\n",
            "✅ Stored batch 253/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 254/365...\n",
            "✅ Stored batch 254/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 255/365...\n",
            "✅ Stored batch 255/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 256/365...\n",
            "✅ Stored batch 256/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 257/365...\n",
            "✅ Stored batch 257/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 258/365...\n",
            "✅ Stored batch 258/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 259/365...\n",
            "✅ Stored batch 259/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 260/365...\n",
            "✅ Stored batch 260/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 261/365...\n",
            "✅ Stored batch 261/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 262/365...\n",
            "✅ Stored batch 262/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 263/365...\n",
            "✅ Stored batch 263/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 264/365...\n",
            "✅ Stored batch 264/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 265/365...\n",
            "✅ Stored batch 265/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 266/365...\n",
            "✅ Stored batch 266/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 267/365...\n",
            "✅ Stored batch 267/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 268/365...\n",
            "✅ Stored batch 268/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 269/365...\n",
            "✅ Stored batch 269/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 270/365...\n",
            "✅ Stored batch 270/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 271/365...\n",
            "✅ Stored batch 271/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 272/365...\n",
            "✅ Stored batch 272/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 273/365...\n",
            "✅ Stored batch 273/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 274/365...\n",
            "✅ Stored batch 274/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 275/365...\n",
            "✅ Stored batch 275/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 276/365...\n",
            "✅ Stored batch 276/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 277/365...\n",
            "✅ Stored batch 277/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 278/365...\n",
            "✅ Stored batch 278/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 279/365...\n",
            "✅ Stored batch 279/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 280/365...\n",
            "✅ Stored batch 280/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 281/365...\n",
            "✅ Stored batch 281/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 282/365...\n",
            "✅ Stored batch 282/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 283/365...\n",
            "✅ Stored batch 283/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 284/365...\n",
            "✅ Stored batch 284/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 285/365...\n",
            "✅ Stored batch 285/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 286/365...\n",
            "✅ Stored batch 286/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 287/365...\n",
            "✅ Stored batch 287/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 288/365...\n",
            "✅ Stored batch 288/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 289/365...\n",
            "✅ Stored batch 289/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 290/365...\n",
            "✅ Stored batch 290/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 291/365...\n",
            "✅ Stored batch 291/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 292/365...\n",
            "✅ Stored batch 292/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 293/365...\n",
            "✅ Stored batch 293/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 294/365...\n",
            "✅ Stored batch 294/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 295/365...\n",
            "✅ Stored batch 295/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 296/365...\n",
            "✅ Stored batch 296/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 297/365...\n",
            "✅ Stored batch 297/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 298/365...\n",
            "✅ Stored batch 298/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 299/365...\n",
            "✅ Stored batch 299/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 300/365...\n",
            "✅ Stored batch 300/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 301/365...\n",
            "✅ Stored batch 301/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 302/365...\n",
            "✅ Stored batch 302/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 303/365...\n",
            "✅ Stored batch 303/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 304/365...\n",
            "✅ Stored batch 304/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 305/365...\n",
            "✅ Stored batch 305/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 306/365...\n",
            "✅ Stored batch 306/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 307/365...\n",
            "✅ Stored batch 307/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 308/365...\n",
            "✅ Stored batch 308/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 309/365...\n",
            "✅ Stored batch 309/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 310/365...\n",
            "✅ Stored batch 310/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 311/365...\n",
            "✅ Stored batch 311/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 312/365...\n",
            "✅ Stored batch 312/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 313/365...\n",
            "✅ Stored batch 313/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 314/365...\n",
            "✅ Stored batch 314/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 315/365...\n",
            "✅ Stored batch 315/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 316/365...\n",
            "✅ Stored batch 316/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 317/365...\n",
            "✅ Stored batch 317/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 318/365...\n",
            "✅ Stored batch 318/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 319/365...\n",
            "✅ Stored batch 319/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 320/365...\n",
            "✅ Stored batch 320/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 321/365...\n",
            "✅ Stored batch 321/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 322/365...\n",
            "✅ Stored batch 322/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 323/365...\n",
            "✅ Stored batch 323/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 324/365...\n",
            "✅ Stored batch 324/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 325/365...\n",
            "✅ Stored batch 325/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 326/365...\n",
            "✅ Stored batch 326/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 327/365...\n",
            "✅ Stored batch 327/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 328/365...\n",
            "✅ Stored batch 328/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 329/365...\n",
            "✅ Stored batch 329/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 330/365...\n",
            "✅ Stored batch 330/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 331/365...\n",
            "✅ Stored batch 331/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 332/365...\n",
            "✅ Stored batch 332/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 333/365...\n",
            "✅ Stored batch 333/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 334/365...\n",
            "✅ Stored batch 334/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 335/365...\n",
            "✅ Stored batch 335/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 336/365...\n",
            "✅ Stored batch 336/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 337/365...\n",
            "✅ Stored batch 337/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 338/365...\n",
            "✅ Stored batch 338/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 339/365...\n",
            "✅ Stored batch 339/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 340/365...\n",
            "✅ Stored batch 340/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 341/365...\n",
            "✅ Stored batch 341/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 342/365...\n",
            "✅ Stored batch 342/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 343/365...\n",
            "✅ Stored batch 343/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 344/365...\n",
            "✅ Stored batch 344/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 345/365...\n",
            "✅ Stored batch 345/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 346/365...\n",
            "✅ Stored batch 346/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 347/365...\n",
            "✅ Stored batch 347/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 348/365...\n",
            "✅ Stored batch 348/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 349/365...\n",
            "✅ Stored batch 349/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 350/365...\n",
            "✅ Stored batch 350/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 351/365...\n",
            "✅ Stored batch 351/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 352/365...\n",
            "✅ Stored batch 352/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 353/365...\n",
            "✅ Stored batch 353/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 354/365...\n",
            "✅ Stored batch 354/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 355/365...\n",
            "✅ Stored batch 355/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 356/365...\n",
            "✅ Stored batch 356/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 357/365...\n",
            "✅ Stored batch 357/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 358/365...\n",
            "✅ Stored batch 358/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 359/365...\n",
            "✅ Stored batch 359/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 360/365...\n",
            "✅ Stored batch 360/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 361/365...\n",
            "✅ Stored batch 361/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 362/365...\n",
            "✅ Stored batch 362/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 363/365...\n",
            "✅ Stored batch 363/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 364/365...\n",
            "✅ Stored batch 364/365\n",
            "⏱️  Waiting 60 seconds to respect API rate limits...\n",
            "Processing batch 365/365...\n",
            "✅ Stored batch 365/365\n",
            "\n",
            "📊 STORAGE SUMMARY:\n",
            "✅ Successful batches: 365\n",
            "❌ Failed batches: 0\n",
            "📄 Total documents attempted: 7294\n",
            "📄 Estimated documents stored: 7300\n",
            "💾 All successful documents persisted to ChromaDB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-15-2923003826.py:667: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vector_store.persist()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div style=\"background-color: #d4edda; border: 1px solid #c3e6cb; color: #155724;\n",
              "                padding: 15px; border-radius: 5px; margin: 10px 0; font-family: Arial;\">\n",
              "        <h3 style=\"margin-top: 0;\">💾 Data Successfully Persisted!</h3>\n",
              "        <p><strong>📍 Location:</strong> /content/drive/MyDrive/ChromaDB_Parkinson_Data</p>\n",
              "        <p><strong>📚 Collection:</strong> parkinsons_complete_kb</p>\n",
              "        <p><strong>📊 Documents:</strong> 7300 chunks stored</p>\n",
              "        <p><strong>🔄 Next Steps:</strong> Your data is now saved locally and ready for Google Drive upload!</p>\n",
              "        <hr style=\"border: 1px solid #c3e6cb;\">\n",
              "        <p style=\"margin-bottom: 0; font-size: 0.9em;\">\n",
              "            <strong>💡 Tip:</strong> You can now upload the entire folder to Google Drive for backup and sharing.\n",
              "        </p>\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎉 SUCCESS: Data persisted to /content/drive/MyDrive/ChromaDB_Parkinson_Data\n",
            "\n",
            "🎉 SUCCESS! Your COMPLETE knowledge base is ready!\n",
            "   📚 Organizations processed: 9\n",
            "   📄 Text document chunks: 6655\n",
            "   🎬 Media document chunks: 639\n",
            "   📊 Total document chunks: 7294\n",
            "   🗄️  ChromaDB collection: parkinsons_complete_kb\n",
            "   💾 Data persisted to: /content/drive/MyDrive/ChromaDB_Parkinson_Data\n",
            "\n",
            "🧪 STEP 2: Testing persistent knowledge base...\n",
            "🔍 CHROMADB INSPECTOR (PERSISTENT VERSION)\n",
            "==================================================\n",
            "📊 Collection Statistics:\n",
            "   collection_name: parkinsons_complete_kb\n",
            "   total_documents: 7294\n",
            "   sample_ids: ['087cf170-cc50-4b41-8b43-99806a9a0bc4', '93880001-2c94-4e9f-9b11-40fd57fc975b', '56121b29-7430-4bff-8bc6-07ef17ef6a0e', 'f4059cc8-e424-4371-afa4-2319300b9366', 'f7ccf8d4-e47e-40a6-9e64-7c65de56638c']\n",
            "   persistent_directory: /content/drive/MyDrive/ChromaDB_Parkinson_Data\n",
            "\n",
            "🏢 Organizations in Database:\n",
            "   1. American Parkinson Disease Association\n",
            "   2. Michael J. Fox Foundation\n",
            "   3. PMD Alliance\n",
            "   4. Parkinson Canada\n",
            "   5. Parkinson's Foundation\n",
            "   6. Parkinson's UK\n",
            "   7. ParkinsonNet\n",
            "\n",
            "🔍 Sample Search Results:\n",
            "\n",
            "Query: 'What are the symptoms of Parkinson's disease?'\n",
            "   1. American Parkinson Disease Association - Score: 0.405\n",
            "      Type: web_page\n",
            "      __Search Search for: # Symptoms of Parkinson’s APDA __Parkinson’s Disease __ Symptoms of Parkinson’s ## **Common Symptoms of Parkinson’s Disease** Below, we discuss the most common Parkinson’s disease...\n",
            "   2. Parkinson's Foundation - Score: 0.432\n",
            "      Type: web_page\n",
            "      ## Breadcrumb * Home * Understanding Parkinson's\n",
            "\n",
            "## Breadcrumb * Home * Understanding Parkinson's\n",
            "\n",
            "#  10 Early Signs  Know how to recognize the most common early symptoms of Parkinson's.  Lady sittin...\n",
            "\n",
            "Query: 'How to manage tremor?'\n",
            "   1. Parkinson's UK - Score: 0.469\n",
            "      Type: web_page\n",
            "      If you want information about other issues like pain, fatigue and anxiety and depression, see our non-motor symptoms page. Read more on our magazine Managing Parkinson's motor symptoms Specialist phys...\n",
            "   2. Parkinson's Foundation - Score: 0.487\n",
            "      Type: web_page\n",
            "      such as levodopa, do not work to control tremor, other medications are sometimes used. For example, **anticholinergics** can be helpful for tremor. These medications improve Parkinson's symptoms by bl...\n",
            "\n",
            "Query: 'exercise therapy'\n",
            "   1. Michael J. Fox Foundation - Score: 0.587\n",
            "      Type: web_page\n",
            "      and may even slow or stop disease progression. Research is ongoing to understand whether and how exercise confers neuroprotection. Please see [add topic page] for more information on discussing an ind...\n",
            "   2. American Parkinson Disease Association - Score: 0.673\n",
            "      Type: web_page\n",
            "      and then gives recommendations on a safe diet to eat. A course of swallow therapy can help improve a person’s swallow to make eating safer and more enjoyable. ### Exercise Instructors: Exercise instru...\n",
            "\n",
            "📄 Text Content Search:\n",
            "   1. Parkinson's Foundation - web_page\n",
            "      There is no one-size-fits all treatment for Parkinson’s. Rather, treatment should be tailored to an ...\n",
            "   2. Parkinson's Foundation - web_page\n",
            "      advanced treatment options? Surgery can be an effective option for managing and improving many movem...\n",
            "   3. American Parkinson Disease Association - web_page\n",
            "      These procedures can be helpful for patients who have significant motor fluctuations in which medica...\n",
            "\n",
            "🎬 Media Content Search:\n",
            "📁 Using persistent directory: /content/drive/MyDrive/ChromaDB_Parkinson_Data\n",
            "   1. [VIDEO] VideosLet’s Keep Moving With APDA: Physical Therapy & Exercise\n",
            "      Organization: American Parkinson Disease Association\n",
            "      URL: https://www.apdaparkinson.org/videos/lets-keep-moving-physical-therapy-exercise/\n",
            "   2. [VIDEO] Watch Teresa Ellis’ presentation onExercise and Parkinson’s\n",
            "      Organization: American Parkinson Disease Association\n",
            "      URL: https://www.apdaparkinson.org/resources-support/educational-video-library/#exercise\n",
            "   3. [VIDEO] VideosLet’s Keep Moving: FAQ’s About Exercise & Physical Therapy\n",
            "      Organization: American Parkinson Disease Association\n",
            "      URL: https://www.apdaparkinson.org/videos/lets-keep-moving-faqs-confirmation/\n",
            "\n",
            "💾 Exporting sample data to persistent directory...\n",
            "✅ Sample data exported to /content/drive/MyDrive/ChromaDB_Parkinson_Data/complete_knowledge_base_sample.json\n",
            "\n",
            "📤 Preparing for Google Drive upload...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div style=\"background-color: #e7f3ff; border: 1px solid #b8daff; color: #004085;\n",
              "                padding: 20px; border-radius: 5px; margin: 10px 0; font-family: Arial;\">\n",
              "        <h3 style=\"margin-top: 0;\">📤 Ready for Google Drive Upload!</h3>\n",
              "\n",
              "        <h4>📁 Your persistent data location:</h4>\n",
              "        <code style=\"background-color: #f8f9fa; padding: 5px; border-radius: 3px;\">/content/drive/MyDrive/ChromaDB_Parkinson_Data</code>\n",
              "\n",
              "        <h4>🚀 Upload Steps:</h4>\n",
              "        <ol>\n",
              "            <li><strong>Compress the folder:</strong> Right-click on the ChromaDB_Parkinson_Data folder and create a ZIP file</li>\n",
              "            <li><strong>Upload to Google Drive:</strong> Upload the ZIP file to your Google Drive</li>\n",
              "            <li><strong>Share with team:</strong> Share the folder with team members if needed</li>\n",
              "        </ol>\n",
              "\n",
              "        <h4>💡 Alternative - Direct Google Drive mount (in Colab):</h4>\n",
              "        <p>If you're in Google Colab, you can mount Google Drive and save directly there!</p>\n",
              "\n",
              "        <h4>📋 What's included in your persistent data:</h4>\n",
              "        <ul>\n",
              "            <li>🗄️ Complete ChromaDB vector database</li>\n",
              "            <li>📊 All document embeddings</li>\n",
              "            <li>🔍 Searchable knowledge base</li>\n",
              "            <li>📁 Collection metadata</li>\n",
              "        </ul>\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 Persistent data ready at: /content/drive/MyDrive/ChromaDB_Parkinson_Data\n",
            "\n",
            "🎉 Pipeline complete! Your persistent knowledge base is ready for use and Google Drive upload.\n"
          ]
        }
      ]
    }
  ]
}